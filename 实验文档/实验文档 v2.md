# HS‑MAD 实验代码指导文档（面向 Codex 的可执行工程规范）

> 本文档将技术文档中的算法与工程实现一一对齐，给出可直接落地的文件/类/接口规范、模块协作与数据流、性能与规范要求，以及需要由人类工程师实现的关键部件说明。所有设计均严格遵循 HS‑MAD 的核心思想：**分层符号对齐 + 多分辨率条件注入（MRCI） + 跳跃连接调制（SCM） + 解耦 CFG**。关键参数（如 `R_VAE=320`、`R1=4`、`R2=4`、SRM/Conformer/HSE/SCM/MRCI/Decoupled‑CFG 的职责与位置、评测集与指标等）均来自技术文档并在实现中显式落地。

---

## 0. 项目范围与目标

- **目标**：在 1D LDM 潜在空间内进行符号到音频（MIDI→Audio）高保真、强结构可控的音乐生成，实现 **HSE（分层符号编码器） → MRCI‑U‑Net（多分辨率注入） → SCM（跳跃连接调制） → 解耦 CFG** 的完整训练与推理流水线。
- **核心承诺**：与 U‑Net 的**多分辨率感受野分层**严格对齐（`T_H / T_M / T_L`），将 **事件/局部/全局**特征分别注入 **高/中/低**分辨率层，并在**跳跃连接**上用 **GatedTCA** 做零初始化门控调制以锐化瞬态与校正时序。
- **训练与评测**：MAESTRO v3 / Slakh2100 + FAD/MUSHRA 与结构一致性指标（Note‑F1, TAE, ODD, BAC）以及 CLAP 分数；包含必要的消融实验（SCM/MRCI 协同）。

---

## 1. 顶层文件组织结构

> 目录结构尽量“**单一职责 + 清晰分层**”，便于 Codex 逐模块生成与替换；同时满足可扩展与可复现实验要求。

```
hs_mad/
├── README.md
├── LICENSE
├── CITATIONS.bib
├── requirements.txt
├── pyproject.toml                  # 工程/格式/lint 配置（可选 .flake8/.ruff.toml）
├── .pre-commit-config.yaml         # 代码质量钩子（isort/black/ruff）
├── configs/
│   ├── data.yaml                   # 数据与预处理（Maestro/Slakh、SR、分段长度等）
│   ├── model.yaml                  # HSE/UNet/SCM/MRCI/Guidance 超参
│   ├── train.yaml                  # 优化器、LR、EMA、AMP、并行、日志
│   ├── infer.yaml                  # 采样、解耦CFG权重、导出设置
│   └── eval.yaml                   # 指标与基线配置
├── scripts/
│   ├── prepare_datasets.sh         # 下载/校验/组织数据
│   ├── cache_features.py           # 预渲染 SRM / 缓存 HSE 特征（可选）
│   └── bench_infer.py              # 推理吞吐/延迟基准
├── src/
│   └── hs_mad/
│       ├── __init__.py
│       ├── utils/
│       │   ├── seed.py             # 随机性控制/复现
│       │   ├── distributed.py      # DDP/策略封装
│       │   ├── logging.py          # 结构化日志/事件统计
│       │   ├── io.py               # I/O、路径、缓存
│       │   ├── midi.py             # MIDI 解析工具
│       │   ├── audio.py            # 音频编解码/分段/重采样
│       │   ├── timing.py           # 性能计时/分析
│       │   └── viz.py              # 可视化（谱线/起始点/节拍等）
│       ├── data/
│       │   ├── datasets/
│       │   │   ├── maestro.py
│       │   │   └── slakh.py
│       │   ├── srm/
│       │   │   ├── renderer.py     # SRM：Soft Piano Roll 渲染 + 同步对齐
│       │   │   └── features.py     # SRM特征缓存与 D_in 组合
│       │   └── datamodules.py      # DataLoader/Collate（时间对齐批处理）
│       ├── modules/
│       │   ├── codecs/
│       │   │   ├── dac_wrapper.py  # DAC 编解码器封装（latent<->wave）
│       │   │   └── clap_encoder.py # CLAP/风格编码器封装
│       │   ├── hse/
│       │   │   ├── conformer.py    # Conformer 块
│       │   │   ├── aux_losses.py   # 和声/节拍/速度/调性辅助任务
│       │   │   └── hse.py          # 分层符号编码器（SRM+HFE+对齐下采样）
│       │   ├── unet/
│       │   │   ├── attention.py    # GatedTCA / CrossAttn / AdaLN
│       │   │   ├── blocks.py       # 1D U-Net 基本残差/上采样/下采样块
│       │   │   ├── scm.py          # 跳跃连接调制（零初始化门控）
│       │   │   └── mrci_unet.py    # 解码器侧 MRCI + SCM 协作
│       │   ├── diffusion/
│       │   │   ├── scheduler.py    # 训练/采样噪声调度（epsilon-pred）
│       │   │   ├── loss.py         # LDM 损失/权重
│       │   │   └── sampler.py      # 解耦 CFG 组合与推理
│       │   └── guidance/
│       │       └── decoupled_cfg.py# 结构/风格独立控制
│       ├── train/
│       │   ├── optim.py            # AdamW/权重衰减/梯度裁剪
│       │   ├── lr_schedules.py     # 线性预热+余弦退火
│       │   ├── hooks.py            # EMA/检查点/评估回调
│       │   └── trainer.py          # 训练循环（AMP/DDP/GradAccum）
│       ├── eval/
│       │   ├── metrics/
│       │   │   ├── fad.py          # FAD 计算
│       │   │   ├── clap_score.py   # CLAP 相似度
│       │   │   ├── onset_beat.py   # ODD/BAC（Madmom）
│       │   │   ├── amt_mt3.py      # MT3 转录 Note-F1/TAE
│       │   │   └── structural.py   # 统一指标聚合与报表
│       │   └── eval_loop.py        # 批量评测/统计
│       └── cli/
│           ├── train.py            # 启动训练
│           ├── sample.py           # 推理与音频导出
│           └── eval.py             # 指标评测入口
└── tests/
    ├── test_alignment.py           # T_H/T_M/T_L 精确对齐检验
    ├── test_srm.py                 # SRM 渲染数值/形状/一致性
    ├── test_hse_shapes.py          # HSE 输出分层尺寸/辅助任务梯度
    ├── test_unet_scm.py            # SCM 零初始化/门控有效性
    ├── test_diffusion_cfg.py       # 解耦CFG 数值等价性
    └── test_e2e_minimal.py         # 端到端 5s 小样本冒烟测试
```

---

## 2. 模块与类：职责、接口与协作

### 2.1 数据与对齐

**关键对齐约束**（技术文档硬性要求，必须测试覆盖）：  
- 音频 44.1kHz，经 **DAC** 压缩到潜在序列，时间压缩因子 `R_VAE=320`，潜在帧率 `SR_latent≈44_100/320≈137.8 Hz`；30s 片段得到 `T_H=⌊30×137.8⌋=4134`。
- U‑Net 分辨率：`T_H`（输入/输出，高分辨）→ 下采样 `R1=4` 得 `T_M` → 再下采样 `R2=4` 得 `T_L`；**HSE 下采样因子与 U‑Net 精确匹配**。

**类：`SyncRenderingModule`（SRM）** — *src/hs_mad/data/srm/renderer.py*  
- **职责**：将异步 MIDI 事件渲染为**软钢琴卷帘**（Onset/Sustain/Velocity 多通道），对齐到 `T_H`，高斯核可微平滑，输出 `S_sync ∈ ℝ^{B×D_in×T_H}`。
- **接口**：
  ```python
  class SyncRenderingModule(nn.Module):
      def __init__(self, d_in: int = 384,
                   sigma_on_ms: float = 3.6, sigma_off_ms: float = 3.6,
                   sr_latent: float = 137.8):
          ...
      def render(self, midi_batch: List[MidiLike], dur_sec: float) -> torch.Tensor:
          """返回形状 [B, D_in, T_H]，T_H=floor(dur_sec*sr_latent)"""
  ```
- **协作**：供 HSE 的 Stage‑1 直接消费；可选 **缓存**到磁盘以避免重复渲染（`features.py`）。

**类：`MaestroDataset` / `SlakhDataset`** — *src/hs_mad/data/datasets/*  
- **职责**：提供（音频, MIDI, 元数据）对齐样本；分段切片到 30s；返回必要字段：`wave`, `midi`, `dur`, `style_text/ref_audio`（可选）。
- **接口**：
  ```python
  class MaestroDataset(Dataset):
      def __getitem__(self, i) -> Dict[str, Any]:
          return {"wave": Tensor[1, T_wave],
                  "midi": MidiLike,
                  "dur_sec": float,
                  "style": {"text": Optional[str], "ref": Optional[Tensor]}}
  ```
- **协作**：`datamodules.py` 负责 Collate，统一 `T_H` 并送入 HSE/DAC。

### 2.2 分层符号编码器（HSE）

**类：`HierarchicalSymbolicEncoder`** — *src/hs_mad/modules/hse/hse.py*  
- **职责**：`MIDI → S_sync (T_H)` → 三阶段 Conformer 提取 `F_event (T_H)`, `F_local (T_M)`, `F_global (T_L)`，并在阶段间用**精心设计的步长 Conv1d**实现**无混叠对齐下采样**（*kernel=2R, stride=R, padding=R//2*）。同时计算**层级辅助任务**（和声/节拍/速度/调性）。
- **关键维度**：`D_event=512, D_local=1024, D_global=1536`。
- **接口（核心）**：
  ```python
  class HierarchicalSymbolicEncoder(nn.Module):
      def __init__(self, R1=4, R2=4, d_in=384,
                   d_event=512, d_local=1024, d_global=1536,
                   n_blocks=(6,6,6), conformer_cfg=...):
          ...
      def forward(self, midi: MidiLike, dur_sec: float) -> Tuple[Dict[str, Tensor], Dict[str, Tensor]]:
          """
          returns
            feats = {"event":  [B, T_H, D_event],
                     "local":  [B, T_M, D_local],
                     "global": [B, T_L, D_global]}
            aux_losses = {"harm": Tensor[], "beat": Tensor[], "tempo": Tensor[], "key": Tensor[]}
          """
  ```
- **协作**：其输出直接按分辨率喂入 **MRCI‑U‑Net** 对应层；`aux_losses` 以 `λ_aux=0.1` 加入总损失。

### 2.3 1D MRCI‑U‑Net 与 SCM

**类：`MRCIUNet1D`** — *src/hs_mad/modules/unet/mrci_unet.py*  
- **职责**：在 **DAC 潜在空间**对噪声进行去噪；**解码器每层**执行：  
  1) **SCM**：对**跳跃连接输入** `Z_skip,l` 以 **GatedTCA (W_zero=0)** 做调制 → 瞬态锐化/时序校正；  
  2) 主干卷积/ResBlock；AdaLN 注入时间步与**风格向量**；  
  3) **MRCI**：用**标准 TCA**把**同层级符号特征** `F_l` 注入主干输出（内容引导）。  
  **F_global→瓶颈**, **F_local→中分辨率**, **F_event→高分辨率**（明确的层级映射）。
- **接口**：
  ```python
  class MRCIUNet1D(nn.Module):
      def forward(self, z_t: Tensor[B,C,T_H], t_embed: Tensor[B,D_t],
                  cond: Dict[str, Tensor],   # {"event":[B,T_H,D_e], "local":[B,T_M,D_l], "global":[B,T_L,D_g]}
                  style: Optional[Tensor[B,D_s]]) -> Tensor[B,C,T_H]:
          ...
  ```
- **类：`GatedTemporalCrossAttention`** — *attention.py*  
  - **职责**：`Q=Z_skip`, `K,V=F_l`；输出投影 `W_zero` **零初始化**，训练初期回退到恒等映射，保证稳定。
  - **接口**：
    ```python
    class GatedTemporalCrossAttention(nn.Module):
        def __init__(self, q_dim, kv_dim, n_heads, zero_init=True): ...
        def forward(self, q: Tensor[B,T,Cq], kv: Tensor[B,T,Dk]) -> Tensor[B,T,Cq]: ...
    ```
- **类：`StandardCrossAttention`** — *attention.py*  
  - **职责**：标准 TCA，用于 MRCI 内容引导路径（解码器主干输出上）。
- **类：`SCMDecoderBlock`** — *scm.py*  
  - **职责**：封装“SCM→融合→UNetBlock→AdaLN→MRCI”的**解码器单元**。提供**可插拔**的 `F_l`。

### 2.4 扩散调度与解耦 CFG

**类：`DiffusionScheduler`** — *diffusion/scheduler.py*  
- **职责**：epsilon‑pred 训练噪声调度（支持线性/余弦/EDM 近似），提供 `add_noise`, `step`。  
- **类：`DecoupledCFG`** — *guidance/decoupled_cfg.py*  
  - **职责**：实现 **结构/风格解耦的加性 CFG**：
    \[
    \epsilon_{final}=\epsilon(z,\varnothing_H,\varnothing_S)
    + w_H(\epsilon(z,C_H,\varnothing_S)-\epsilon(z,\varnothing_H,\varnothing_S))
    + w_S(\epsilon(z,\varnothing_H,C_S)-\epsilon(z,\varnothing_H,\varnothing_S))
    \]
    提供三路前向并组合输出，`w_H,w_S` 可独立调节。
  - **接口**：
    ```python
    class DecoupledCFG(nn.Module):
        def forward(self, unet, z_t, t, cond_H, cond_S, w_H: float, w_S: float) -> Tensor:
            ...
    ```

### 2.5 训练与评测

**类：`Trainer`** — *train/trainer.py*  
- **职责**：DDP/AMP/GradAccum/EMA；总损失 `L_total = L_LDM + Σ λ_aux L_aux`；按 `cfg` 日志与保存。**峰值 LR=1e-4, 线性预热 1e4 步, 余弦衰减, 全局 batch=128, EMA=0.9999**。

**评测模块** — *eval/*  
- **FAD**、**MUSHRA 协议**（半自动）、**Note‑F1/TAE**（MT3）、**ODD/BAC**（Madmom）、**CLAP 分数**；对比**Baseline‑Global/Flat/AR/Dense**与**消融**（仅SCM/仅MRCI）。

---

## 3. 模块之间的协作与数据流（训练/推理）

### 3.1 训练前向（单步）

1. **数据层**：`Dataset` 取样 → `SRM.render()` 得 `S_sync[B, D_in, T_H]`。  
2. **HSE**：`S_sync → (F_event[T_H], F_local[T_M], F_global[T_L])` + `aux_losses`。  
3. **编码器**：`wave → DAC.encode() → z_0 [B,C,T_H]`。  
4. **加噪**：`z_t = scheduler.add_noise(z_0, t)`。  
5. **MRCI‑U‑Net**：  
   - **SCM**：在每个解码层，对 `Z_skip` 执行 `GatedTCA(Z_skip, F_l)`（`l=H/M`）。  
   - **AdaLN**：注入 `t_embed` 与 `style_vec`（来自 CLAP 文本/音频）。  
   - **MRCI**：`Standard TCA(Z_main, F_l)` 注入语义，引导内容。  
6. **损失**：`L_LDM = ||ε_pred - ε||^2`（或相容变体）+ `λ_aux Σ L_aux` → 反向。  
> 以上流程与技术文档“**HSE 对齐 → SCM 锐化 → MRCI 内容引导**”完全一致。

### 3.2 采样（解耦 CFG）

- 对同一 `z_t` 分别前向：(**空/结构/风格**)三路，再按 `w_H, w_S` **加性组合**得到 `ε_final`，推进采样；最终经 **DAC.decode()** 还原波形。

---

## 4. 代码规范（Python）

- **风格**：`black` + `ruff`；行宽 100；**中文文档注释**解释“设计动机/对齐关系/数值范围/形状”。  
- **类型**：`from __future__ import annotations` + `typing`（`TensorAlias = torch.Tensor`）。  
- **文档**：每个公共类/方法用 **Google 风格 docstring** + 公式/形状示例。  
- **命名**：时间维一律 `T_*`；分辨率后缀 `*_H/*_M/*_L`；特征字典键固定 `{event, local, global}`。  
- **确定性**：`seed_everything(seed)`；固定 `cudnn.benchmark=False`；必要时启用 `deterministic_algorithms(True)`。  
- **错误处理**：显式断言对齐：`assert feats["event"].size(1) == T_H` 等。  
- **日志/可视化**：训练日志中每 N step 绘制 `S_sync` 与 `Onset/Beat` 对齐图、`SCM` 门控强度曲线。

---

## 5. 关键接口示例（含中文注释）

```python
# src/hs_mad/modules/unet/attention.py
class GatedTemporalCrossAttention(nn.Module):
    """
    基于缩放点积注意力的时序交叉注意力，输出投影零初始化以门控增益从0开始。
    设计动机：在跳跃连接上进行“可学习的微校正”，初期不破坏原始高频通路，稳定训练。
    Args:
        q_dim: Q通道（来自Z_skip）
        kv_dim: K/V通道（来自同分辨率符号特征 F_l）
        n_heads: 注意力头数
        zero_init: 是否对输出线性层做零初始化（SCM 要求=True）
    Shape:
        q: [B, T_l, q_dim], kv: [B, T_l, kv_dim] → out: [B, T_l, q_dim]
    """
    ...
```

```python
# src/hs_mad/modules/hse/hse.py
class HierarchicalSymbolicEncoder(nn.Module):
    """
    将 S_sync[T_H] 通过三阶段 Conformer 提取 F_event/F_local/F_global，并用步长卷积实现
    与 U-Net 对齐的下采样（kernel=2R, stride=R, padding=R//2），降低混叠并精确中心对齐。
    同时输出层级辅助任务损失（局部: 和声/节拍；全局: 速度/调性）。
    """
    ...
```

```python
# src/hs_mad/modules/unet/mrci_unet.py（解码器单块伪码）
class SCMDecoderBlock(nn.Module):
    def forward(self, z_in, z_skip, F_l, t_emb, style_vec):
        # 1) SCM：在跳跃连接上用 GatedTCA 做时序锐化/校正（零初始化门控）
        z_skip_mod = z_skip + self.gated_tca(z_skip.transpose(1,2), F_l).transpose(1,2)
        # 2) 融合 + 主干卷积
        z = self.fuse(torch.cat([z_in, z_skip_mod], dim=1))
        z = self.resblocks(z, t_emb)
        # 3) 风格注入（AdaLN）
        z = self.adaln(z, style_vec, t_emb)
        # 4) MRCI：基于标准 TCA 的内容引导（同层级 F_l 注入）
        z = z + self.cross_attn(z.transpose(1,2), F_l).transpose(1,2)
        return z
```

---

## 6. 性能与工程最佳实践

- **计算对齐**：SRM/HSE 的下采样核满足 `kernel=2R`、`stride=R`、`padding=R//2`，在张量级单元测试中检查**中心对齐**误差 < 0.5 帧。
- **内存/吞吐**：  
  - 使用 **AMP (autocast+GradScaler)**；**梯度检查点**在 Conformer 与 U‑Net 深层启用；  
  - `torch.compile(dynamic=True)`（若驱动允许）；  
  - 预渲染/缓存 `S_sync` 与中间 HSE 特征以复用；  
  - DataLoader 绑核，`prefetch_factor>2`，`pin_memory=True`；  
  - 注意力实现优先调用 `scaled_dot_product_attention`（Flash‑Attention 自动分支）。  
- **数值稳定**：SCM 的 `W_zero` 零初始化，避免早期破坏；AdaLN 的尺度/偏置初始为小值；扩散步长余弦衰减。
- **可视化诊断**：  
  - 绘制**时序漂移**曲线（每 5s 统计 TAE）；  
  - 展示 SCM 门控输出的 L2 能量热图与 Onset 对齐；  
  - 记录 `BAC` 的长时段节拍稳定度。

---

## 7. 依赖包管理（`requirements.txt` 建议）

> 版本可按实际 CUDA/平台微调；以下为**可复现实验**的保守建议。

```
torch==2.3.*
torchaudio==2.3.*
einops>=0.7.0
numpy>=1.26
scipy>=1.11
librosa>=0.10.1
numba>=0.58
madmom>=0.16.1        # ODD/BAC（起始/节拍检测）
soundfile>=0.12
tqdm>=4.66
pyyaml>=6.0.1
pydantic>=2.8
omegaconf>=2.3.0
accelerate>=0.33
huggingface_hub>=0.23
laion-clap>=1.1.5     # 风格编码器（CLAP）
essentia>=2.1b6       # 可用于 FAD/VGGish 特征（或替换为其他实现）
protobuf<5
pandas>=2.2
matplotlib>=3.8
webrtcvad>=2.0.10     # （可选）静音片段剔除
```

> **无需重写的模块**：DAC 编解码器、CLAP 编码器、MT3（用于评测）、Madmom（起始/节拍）、FAD 实现——**直接引用/封装现有实现**即可，工程仅需提供 *wrapper* 与缓存/批处理逻辑。

---

## 8. 实验配置（与技术文档一一对应）

- **音频编解码器**：DAC@44.1kHz；`R_VAE=320`；`SR_latent≈137.8Hz`；片段 30s → `T_H=4134`。
- **HSE**：`D_in=384`；Conformer（8 头，FFN 2048，Conv 核 31；三阶段各 6 块）；`D_event=512, D_local=1024, D_global=1536`；`R1=4, R2=4`。
- **U‑Net**：High/Mid/Bottleneck 通道 `{512,1024,1536}`；**SCM 应用于 High/Mid**；GatedTCA 采用 `W_zero=0`。
- **优化**：AdamW；峰值 LR=1e-4；线性预热 10k；余弦衰减；Batch=128；Steps=1M；EMA=0.9999；CFG Dropout `p_H=p_S=0.1`；`λ_aux=0.1`。
- **数据集**：MAESTRO v3、Slakh2100（对齐 MIDI+音频）。
- **指标**：FAD、MUSHRA、Note‑F1/TAE（MT3）、ODD/BAC（Madmom）、CLAP 分数。
- **对比/消融**：Baseline‑Global/Flat/AR/Dense；仅SCM/仅MRCI/全量。

---

## 9. 需要人类工程师手写或深度参与的模块（量力而行清单）

> 这些模块**不建议**由 Codex 一次性自动生成；请按下述步骤由人类实现或审校。

1. **DAC 封装（`dac_wrapper.py`）**：  
   - 适配具体开源权重/接口；验证 `encode/decode` 与 `R_VAE=320` 一致；构造**批量/流式** API。
2. **CLAP/MT3/Madmom/FAD 封装**：  
   - 统一 I/O（批量化、FP16 支持、缓存）；与评测脚本打通。
3. **Conformer Block（高质量实现）**：  
   - 使用 `SDPA`/`Flash‑Attn`、深度可分卷积、残差/归一化的**数值细节**需专家把关。
4. **MRCI‑U‑Net 拓扑与对齐**：  
   - 保证各层 `T_l` 与 `F_l` **一一对应**，以及 **SCM→融合→主干→AdaLN→MRCI** 的**顺序**正确（见 §2.3 示例）。
5. **评测基线集成**：  
   - AR/LDM 基线推理脚本统一接口，确保 `Note‑F1/TAE/ODD/BAC` 可横向比较。

> 我将为以上每项给出**可操作步骤**与*checklist*（见下节）。

---

## 10. 人工实现步骤指引（逐步 Checklist）

### 10.1 `dac_wrapper.py`
1. 加载官方/开源 DAC 权重；封装 `encode(wave_44k1)`→`z_latent[C,T_H]` 与 `decode(z)`。  
2. 单元测试：随机正弦 + 白噪声往返重建 SNR；验证 `T_H == floor(dur*SR_latent)`。

### 10.2 `conformer.py`
1. 模块化 `SelfAttn → Conv → FFN` 顺序（或 FFN‑Attn‑Conv‑FFN）；  
2. 支持 `drop_path` 与 **前后归一化一致性**；  
3. `stride conv` 下采样层：核/步长/填充按 `2R/R/R//2`，含**抗混叠**注释。

### 10.3 `mrci_unet.py` & `scm.py`
1. 构造**解码器块**模板（见示例）；  
2. 在 High/Mid 两级启用 SCM，瓶颈/其他级**关闭**；  
3. 断言：`F_l.shape[1] == Z_l.shape[-1]`；SCM 输出线性层权重**全零**初始化测试。

### 10.4 评测封装
1. **MT3**：批量转录，输出 onset/pitch 列表 → Note‑F1/TAE；  
2. **Madmom**：起始/节拍检测 → ODD/BAC；  
3. **FAD/CLAP**：离线提特征 + 统一缓存键（音频哈希）。

---

## 11. 单元测试与验证要点

- **`test_alignment.py`**：验证 `T_H/T_M/T_L` 与理论计算一致；下采样中心对齐误差 < 0.5 帧。
- **`test_srm.py`**：Onset 高斯渲染能量守恒（单位区间内积分近似保持）。
- **`test_hse_shapes.py`**：输出字典键/形状/梯度存在；辅助损失可回传。
- **`test_unet_scm.py`**：SCM 输出层初始为 0，前 100 step 不致崩；逐步学到正增益。
- **`test_diffusion_cfg.py`**：`w_H=0,w_S=0` 时与无条件预测等价；`w_H>0` 提升 ODD/TAE；`w_S>0` 提升 CLAP。
- **`test_e2e_minimal.py`**：5 秒片段端到端正向/反向通过，显存 < 12GB。

---

## 12. 训练/评测脚本用法（CLI）

```bash
# 训练
python -m hs_mad.cli.train --config configs/{data,model,train}.yaml

# 推理/采样（解耦CFG）
python -m hs_mad.cli.sample --ckpt path/to/ckpt.ckpt --config configs/infer.yaml \
       --w_H 2.0 --w_S 1.5 --midi path/to/input.mid --style.text "romantic piano"

# 评测（FAD / Note-F1 / TAE / ODD / BAC / CLAP）
python -m hs_mad.cli.eval --ckpt path/to/ckpt.ckpt --config configs/eval.yaml
```

---

## 13. 配置示例片段（与论文对齐）

```yaml
# configs/model.yaml（要点）
latent:
  R_VAE: 320          # DAC 时间压缩
  sr: 44100
  sr_latent: 137.8125 # 近似值，统一由 dac_wrapper 推导校验
unet:
  ch: {H:512, M:1024, L:1536}
  scm_levels: ["H","M"]  # 只在高/中分辨率启用 SCM
hse:
  d_in: 384
  dims: {event:512, local:1024, global:1536}
  R1: 4
  R2: 4
guidance:
  cfg_dropout: {H:0.1, S:0.1}
  w_H: 2.0
  w_S: 1.5
train:
  lr_peak: 1.0e-4
  warmup_steps: 10000
  ema: 0.9999
  batch_size_global: 128
  lambda_aux: 0.1
```
> 上述数值来自技术文档“实现细节/超参数”节。

---

## 14. 实验设计落地（对照技术文档）

- **SOTA 对比/基线**：Baseline‑Global / Baseline‑Flat / Baseline‑AR / Baseline‑Dense；  
- **关键假设**：HS‑MAD 在结构一致性（Note‑F1/TAE/ODD/BAC）显著优于基线；Dense 低于 MRCI；FAD 优于 AR。  
- **消融**：仅 SCM（无 MRCI）→ Note‑F1 下降；仅 MRCI（无 SCM）→ ODD/TAE 变差、瞬态钝化。  
- **时序漂移分析**：每 5s 统计 TAE，HS‑MAD 趋势平缓。  
> 完全对应技术文档的 §4.3，要保证脚本产出相同的统计与图表。

---

## 15. 常见陷阱与规避

- **分辨率错配**：`F_l` 时间轴与 `Z_l` 未严格对齐会导致注意力退化——务必在 `forward` 中 `assert`。
- **SCM 先后顺序**：SCM 必须**作用于跳跃连接输入，且在融合之前**；若误放在主干输出，会与 MRCI 冲突。
- **辅助任务过强**：`λ_aux` 过大将挤占主任务容量，建议从 `0.1` 起调。
- **CFG 解耦实现错误**：确保三路前向**共享同一 `z_t`/`t`**，仅条件不同；加性组合与技术文档公式一致。

---

## 16. 交付清单（编码阶段验收标准）

- [ ] 目录/文件按 §1 完成并可导入  
- [ ] `SRM.render()` 生成的 `S_sync` 与 `T_H` 严格对齐（单测通过）  
- [ ] `HSE.forward()` 输出三层特征与 `aux_losses`（形状/梯度 OK）  
- [ ] `MRCIUNet1D` 的解码器块顺序符合 **SCM→融合→主干→AdaLN→MRCI**  
- [ ] `DecoupledCFG` 三路/加性组合数值测试通过  
- [ ] 端到端 5s 样本冒烟跑通，显存与时延满足基线  
- [ ] 评测环节能产出技术文档要求的全部指标与图表

---

### 附：最小示例（端到端伪代码，便于 Codex 生成骨架）

```python
# src/hs_mad/cli/train.py
def main(cfg):
    seed_everything(cfg.seed)
    ds = MaestroDataset(cfg.data)  # or SlakhDataset
    dl = DataLoader(ds, batch_size=cfg.train.bs, ...)
    dac = DACWrapper(cfg.latent)
    hse = HierarchicalSymbolicEncoder(...)
    unet = MRCIUNet1D(...)
    sched = DiffusionScheduler(...)
    opt = AdamW(itertools.chain(hse.parameters(), unet.parameters()), lr=cfg.train.lr_peak, ...)
    ema = EMA(unet, decay=cfg.train.ema)

    for step, batch in enumerate(dl):
        wave, midi, dur = batch["wave"], batch["midi"], batch["dur_sec"]
        z0 = dac.encode(wave)                              # [B,C,T_H]
        feats, aux_losses = hse(midi, dur)                 # dict of [B,T_l,D_l]
        t = sched.sample_t(z0.shape[0])
        zt, eps = sched.add_noise(z0, t)
        eps_pred = unet(zt, t_embed(t), feats, style_from(batch["style"]))
        loss = mse(eps_pred, eps) + cfg.train.lambda_aux * sum(aux_losses.values())
        loss.backward(); clip_grad_norm_(...); opt.step(); opt.zero_grad()
        ema.update(unet)
```

---

## 17. 结语

以上实验指导文档将技术文档中的**理论与超参**精确投影到工程落地层面，确保 **文件结构清晰**、**类与接口职责明确**、**模块协作与数据流无二义**、并以**性能/稳定性/可复现**为第一优先级。涉及第三方能力（DAC/CLAP/MT3/Madmom/FAD）的部分，均按“**封装复用**”原则处理，避免重复造轮子，严格贯彻技术文档的设计初衷与对齐要求。