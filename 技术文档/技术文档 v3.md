## HS-MAD：层级符号对齐的音乐音频扩散模型

**(Hierarchical Symbolic-Conditioned Music Audio Diffusion)**

### 摘要

实现高保真度（Fidelity）与精确结构控制（Controllability）的平衡是音乐音频生成领域的核心挑战。潜在扩散模型（LDM）在音质上已达到最先进水平 [Rombach et al., 2022; Liu et al., 2023]，但由于缺乏显式、时间精确对齐的结构化引导，它们在遵循复杂符号指令（如乐谱）时常出现时序漂移和结构失真。本文提出 HS-MAD（Hierarchical Symbolic-Conditioned Music Audio Diffusion），一种新颖的 1D 扩散架构，旨在解决这一矛盾。HS-MAD 的核心洞察在于利用音乐的层级本质（从宏观曲式到微观音符）[Lerdahl & Jackendoff, 1983; Paiement et al., 2009] 与 U-Net 架构的多分辨率特性 [Ronneberger et al., 2015] 之间的同构性（Isomorphism）。我们设计了一个基于 Conformer 的分层符号编码器（HSE），通过一个精确的同步渲染模块（SRM）确保符号特征与 U-Net 潜在空间在时间维度上严格对齐。我们引入了多分辨率条件注入（MRCI）机制，将高层结构信息引导 U-Net 的低分辨率层，将底层时序信息引导高分辨率层。此外，我们创新性地提出了跳跃连接调制（SCM），使用基于零初始化的门控机制（GatedTCA）直接调制 U-Net 的跳跃连接，以显著增强音频瞬态的时序精度。我们还引入了层级辅助任务来强化 HSE 的特征解耦，并提出了“解耦无分类器指导”（Decoupled CFG）策略以实现结构与风格的独立控制。本文提供了详尽的架构设计、算法细节和实现参数，为实现高质量、结构精确可控的音乐生成提供了一个完整可复现的技术方案。

### 1. 引言：动机与理论基础

音乐是一种在时间维度上展现出复杂层级结构的时序信号 [Lerdahl & Jackendoff, 1983]。从微观的音频瞬态、音符事件，到中观的节奏模式、和声进行，再到宏观的曲式结构，不同层次的元素相互依赖、共同作用。生成模型必须同时捕捉这种复杂的结构关系和高保真的声学特性。

近年来，潜在扩散模型（LDM）[Rombach et al., 2022] 在音频生成领域取得了显著进展 [Liu et al., 2023; Sors et al., 2024]。我们采用 1D LDM 框架，直接在神经音频编解码器的潜在空间中操作，因为它在处理音频时序信号，特别是相位和瞬态信息方面，优于 2D（频谱图）表示 [Défossez et al., 2022]。然而，现有的 LDM 在控制机制上通常依赖于全局嵌入向量（例如来自文本的 CLAP 嵌入 [Wu et al., 2023]），这种粗粒度的控制无法精确指定音乐的时序结构。在执行符号到音频（Symbol-to-Audio）合成任务时，将符号信息（如 MIDI）编码后采用“单点注入”策略（通常注入到 U-Net 的瓶颈层）会导致严重的信息瓶颈，导致生成的音频出现时序漂移（Temporal Drift）和细节失真。

HS-MAD 的理论基础源于一个关键洞察：**音乐的层级结构与 U-Net 架构的多分辨率特性之间存在天然的同构性。** U-Net [Ronneberger et al., 2015] 通过下采样路径逐步增大感受野，其深层（低分辨率）负责捕捉全局上下文；通过上采样路径结合跳跃连接，其浅层（高分辨率）负责重建局部细节和精确时序。这与音乐的层级组织高度吻合。

HS-MAD 首次显式地将这两种层级结构进行对齐和映射。我们提出了一种“分而治之”的控制策略：利用高层音乐结构信息引导 U-Net 的深层计算，确保全局连贯性；利用底层音乐细节信息引导 U-Net 的浅层计算，确保微观时序精度。

本文的主要贡献与创新点如下：

1.  **[核心创新] HS-MAD 框架**：提出了一种通过显式对齐音乐层级结构与 1D U-Net 多分辨率特性来实现精确结构控制的音频 LDM 框架。
2.  **[创新点] 分层符号编码器 (HSE) 与同步机制**：设计了一个基于 Conformer [Gulati et al., 2020] 的层级化编码器，结合了精确的同步渲染模块（SRM），以生成与 U-Net 特征图严格时间对齐的多尺度表示。引入了层级辅助任务以强化特征解耦。
3.  **[创新点] 多分辨率条件注入 (MRCI) 与跳跃连接调制 (SCM)**：开发了层级注入策略，并创新性地引入了 SCM，使用基于零初始化的门控机制（GatedTCA）[Zhang et al., 2023] 直接调制 U-Net 的跳跃连接，以显著增强音频瞬态的时序精度。
4.  **[创新点] 解耦无分类器指导 (Decoupled CFG)**：提出了一种基于加性组合 [Liu et al., 2022] 的多模态 CFG [Ho & Salimans, 2022] 策略，允许独立控制结构和风格条件的引导强度。

### 2. HS-MAD 架构详解

HS-MAD 采用 1D LDM 框架，在一个预训练的神经音频编解码器（如 DAC [Kumar et al., 2023]）的潜在空间中进行操作。核心是去噪网络 $\epsilon_\theta$，它由分层符号编码器 (HSE) 和多分辨率条件注入 1D U-Net (MRCI U-Net) 组成。

```
graph TD
    subgraph "HS-MAD 详细架构 (1D LDM)"
        direction TB

        %% 1. 输入与编码
        Symbolic_Input("符号输入 S (MIDI)") --> HSE
        Style_Input("风格输入 T (文本/音频参考)") --> Style_Encoder("风格编码器 (CLAP/T5)") --> F_style("F_style (C_S)")

        subgraph HSE [分层符号编码器 (HSE)]
            direction TB
            S_in("S") --> SRM("同步渲染模块 (SRM)") --> S_sync("同步表示 S_sync (T_H)")
            S_sync --> Conformer_1("Stage 1: Event") --> F_event("F_event (T_H)")
            F_event -- "Strided Conv1D (R1)" --> Conformer_2("Stage 2: Local") --> F_local("F_local (T_M)")
            F_local -- "Strided Conv1D (R2)" --> Conformer_3("Stage 3: Global") --> F_global("F_global (T_L)")
            
            %% Auxiliary Tasks
            F_local -- "Aux Head" --> L_aux_local("L_aux (和声/节拍)")
            F_global -- "Aux Head" --> L_aux_global("L_aux (速度/调性)")
        end
        
        C_H("C_H = {F_event, F_local, F_global}")

        %% 2. MRCI 1D U-Net (去噪网络 ε_θ)
        Noisy_Latent_zt("带噪潜在表示 z_t (T_H)") --> MRCI_U_Net

        subgraph "MRCI 1D U-Net"
            direction TB
            Input_zt("z_t") --> U_Down_1("下采样块 1 (T_H)")
            U_Down_1 -- "Downsample (R1)" --> U_Down_2("下采样块 2 (T_M)")
            U_Down_2 -- "Downsample (R2)" --> U_Bottleneck("瓶颈层 (T_L)")

            U_Bottleneck --> U_Up_2("上采样块 2 (T_M)")
            U_Up_2 --> U_Up_1("上采样块 1 (T_H)")
            U_Up_1 --> Output_e("预测噪声 ε_θ")

            %% 跳跃连接调制 (SCM) (创新点)
            U_Down_1 -- "Z_skip_1" --> Skip_Mod_1("SCM (GatedTCA)")
            F_event -- "Condition" --> Skip_Mod_1
            Skip_Mod_1 -- "Z'_skip_1" --> U_Up_1

            U_Down_2 -- "Z_skip_2" --> Skip_Mod_2("SCM (GatedTCA)")
            F_local -- "Condition" --> Skip_Mod_2
            Skip_Mod_2 -- "Z'_skip_2" --> U_Up_2

            %% 多分辨率条件注入 (MRCI)
            F_event -- "TCA" --> U_Down_1 & U_Up_1
            F_local -- "TCA" --> U_Down_2 & U_Up_2
            F_global -- "TCA" --> U_Bottleneck

            %% 风格注入
            F_style -- "AdaLN" --> U_Down_1 & U_Down_2 & U_Bottleneck & U_Up_2 & U_Up_1
        end
    end
```

*图 1：HS-MAD 架构概览。我们采用 1D LDM 框架。HSE 通过 SRM 实现与 U-Net 潜在空间的精确时间同步，并通过 Conformer 提取多尺度特征。MRCI 1D U-Net 通过多分辨率条件注入 (MRCI) 和跳跃连接调制 (SCM) 机制，在对应层级利用这些特征进行引导。*

#### 2.1 层级对齐定义

我们使用一个预训练的神经音频编解码器将音频 $X$ 压缩为 1D 潜在表示 $z_0 \in \mathbb{R}^{C \times T_H}$。编解码器具有时间压缩因子 $R_{VAE}$。潜在空间的帧率（分辨率）为 $SR_{latent} = SR / R_{VAE}$。

U-Net 通过下采样操作创建多分辨率的特征图。我们定义三个关键分辨率级别，下采样因子为 $R_1, R_2$：
-   $T_H$ (High-Res)：输入/输出层。
-   $T_M$ (Mid-Res)：中间层，$T_M = T_H / R_1$。
-   $T_L$ (Low-Res)：瓶颈层，$T_L = T_M / R_2$。

HS-MAD 的目标是生成一个与这些分辨率精确对齐的层级化符号条件 $C_H = \{F_{event}, F_{local}, F_{global}\}$。

1.  **$F_{event}$ (事件层, $T_H$)**: 捕捉精确的音符时序、音高、力度和微观时序。
2.  **$F_{local}$ (局部层, $T_M$)**: 捕捉局部和声、节奏模式（节拍）。
3.  **$F_{global}$ (全局层, $T_L$)**: 捕捉宏观曲式结构、全局调性和速度曲线。

#### 2.2 分层符号编码器 (HSE) (创新点)

HSE 的任务是将异步的符号事件 $S$（MIDI）转换为同步对齐、语义解耦的多尺度表示 $C_H$。它由同步渲染模块（SRM）和层级特征提取器（HFE）组成。

##### 2.2.1 同步渲染模块 (SRM)：精确时间对齐

SRM 负责将 MIDI 事件转换为一个同步的输入表示 $S_{sync} \in \mathbb{R}^{D_{in} \times T_H}$。$T_H$ 必须与目标音频时长 $DUR$ 精确匹配：$T_H = \lfloor DUR \times SR_{latent} \rfloor$。

我们将 $S_{sync}$ 实现为一种多通道的“软钢琴卷帘”（Soft Piano Roll）[Hawthorne et al., 2018]。其输入维度 $D_{in}$ 被组织为 $P$ 个音高（例如 128）乘以 $K$ 个通道。我们定义 $K=3$ 个核心通道：

1.  **Onset（起始）信号**：表示音符的起始时间和力度。
2.  **Sustain（持续）信号**：表示音符在起始后是否持续发声。
3.  **Velocity（力度）信号**：在音符持续期间编码其力度值。

**可微平滑渲染**：为了保留微观时序并确保可微性，我们使用高斯核进行渲染。令时间帧索引 $t_h \in [0, T_H-1]$ 对应的实际时间为 $t_{sec} = t_h / SR_{latent}$。对于一个音符 $n$（音高 $p_n$，起始 $t_{start, n}$，结束 $t_{end, n}$，力度 $v_n$）：

Onset 信号渲染为：

$$ S_{sync}[\text{Onset}, p_n, t_h] = v_n \cdot \exp\left(-\frac{(t_{sec} - t_{start, n})^2}{2\sigma_{on}^2}\right) $$

Sustain 信号渲染为（包含平滑的结束衰减，模拟延音效果）：

$$ S_{sync}[\text{Sustain}, p_n, t_h] = \begin{cases} 
      1 & \text{if } t_{start, n} \leq t_{sec} \leq t_{end, n} \\
      \exp\left(-\frac{(t_{sec} - t_{end, n})^2}{2\sigma_{off}^2}\right) & \text{if } t_{sec} > t_{end, n} \\
      0 & \text{otherwise}
   \end{cases}
$$

Velocity 信号等于 $v_n$（归一化到 $[0, 1]$），当且仅当 Sustain 信号非零。$\sigma_{on}$ 和 $\sigma_{off}$ 是控制锐度的超参数（例如，设置为 $1/(2 \cdot SR_{latent})$）。对于复音音乐，同一时间帧的信号进行累加。

##### 2.2.2 基于 Conformer 的层级特征提取器 (HFE)

HFE 从 $S_{sync}$ 中提取层级特征。我们采用 Conformer 架构 [Gulati et al., 2020]，它结合了卷积（捕捉局部时序模式）和 Transformer（捕捉长程依赖）[Vaswani et al., 2017] 的优势。

HFE 由三个阶段组成。关键要求是 HFE 的下采样因子 $R_1, R_2$ 必须与 U-Net 精确匹配。我们使用步长 1D 卷积（Strided Conv1D）进行下采样。为了确保精确对齐并最小化信息损失，我们使用较大的卷积核（例如，Kernel Size = $2 \times R_i$）和匹配的步长（Stride = $R_i$），并设置适当的填充（Padding = $R_i // 2$）。

##### 2.2.3 层级辅助任务 (Hierarchical Auxiliary Tasks) (创新点)

为了引导 HSE 学习到具有音乐意义的层级表示，并促进特征空间的解耦，我们引入了辅助监督信号。这些任务的标签可以使用 MIR 工具（如 music21, Madmom [Böck et al., 2016]）提取，并同步到对应分辨率。

1.  **局部层辅助任务 ($F_{local}$, $T_M$)**:
    -   **和声预测 (Harmony Prediction)**：预测色度图（Chromagram）或和弦标签。使用交叉熵损失 $L_{Aux\_Harmony}$。
    -   **节拍预测 (Beat Prediction)**：预测节拍和下拍位置。使用二元交叉熵损失 $L_{Aux\_Beat}$。

2.  **全局层辅助任务 ($F_{global}$, $T_L$)**:
    -   **速度预测 (Tempo Prediction)**：预测全局 BPM 或局部速度曲线。使用均方误差损失 $L_{Aux\_Tempo}$。
    -   **调性预测 (Key Prediction)**：预测全局调性。使用交叉熵损失 $L_{Aux\_Key}$。

**算法 1：分层符号编码器 (HSE) 伪代码**

```python
import torch
import torch.nn as nn

class HierarchicalSymbolicEncoder(nn.Module):
    # 伪代码，展示关键结构和维度处理逻辑
    def __init__(self, R_VAE, R_Unet_factors, SR, Dims):
        super().__init__()
        # R_Unet_factors (R1, R2) 必须与 U-Net 匹配
        self.R1, self.R2 = R_Unet_factors
        self.SRM = SymbolicRenderingModule(R_VAE, SR, Dims['in'])
        D_event, D_local, D_global = Dims['event'], Dims['local'], Dims['global']

        # HFE Stages (Conformer Blocks) - 假设输入格式 (B, D, T)
        self.Stage1_Event = ConformerBlocks(Dims['in'], D_event, ...)
        
        # 精确下采样 (Strided Conv1D)
        # Kernel Size=2*R, Padding=R//2 确保感受野覆盖和对齐
        self.Downsample1 = nn.Conv1d(D_event, D_local, kernel_size=2*self.R1, stride=self.R1, padding=self.R1//2)
        self.Stage2_Local = ConformerBlocks(D_local, D_local, ...)
        
        self.Downsample2 = nn.Conv1d(D_local, D_global, kernel_size=2*self.R2, stride=self.R2, padding=self.R2//2)
        self.Stage3_Global = ConformerBlocks(D_global, D_global, ...)

        # 辅助任务头 (略)
        # self.AuxHeads = ...

    def forward(self, S_midi, Audio_Duration, Targets=None):
        # 1. 同步渲染模块 (SRM)
        S_sync = self.SRM(S_midi, Audio_Duration) # (B, D_in, T_H)
        T_H = S_sync.shape[2]

        # 2. 层级特征提取器 (HFE)
        F_event = self.Stage1_Event(S_sync)

        H_mid = self.Downsample1(F_event)
        # 维度断言：确保 T_M ≈ T_H / R1 (实际维度取决于卷积参数)
        # assert H_mid.shape[2] == T_H // self.R1 
        F_local = self.Stage2_Local(H_mid)

        H_global = self.Downsample2(F_local)
        # 维度断言：确保 T_L ≈ T_M / R2
        # assert H_global.shape[2] == (T_H // self.R1) // self.R2
        F_global = self.Stage3_Global(H_global)

        # 3. 辅助任务损失计算 (略，详见实现细节)
        Aux_Losses = {} 

        # 返回时序维度在后的格式 (B, T, D) 以适应 U-Net Cross-Attention
        Features = {
            'event': F_event.transpose(1, 2), 
            'local': F_local.transpose(1, 2), 
            'global': F_global.transpose(1, 2)
        }
        return Features, Aux_Losses
```

#### 2.3 多分辨率条件注入 1D U-Net (MRCI U-Net) (核心创新点)

MRCI U-Net 是去噪网络 $\epsilon_\theta$。我们采用基于 Transformer 块和 1D ResNet 块构建的 U-Net 架构 [Sors et al., 2024]。我们通过时序交叉注意力（TCA）和创新的跳跃连接调制（SCM）将 HSE 的输出注入到 U-Net 的对应层级。

##### 2.3.1 时序交叉注意力 (TCA) 与多分辨率注入 (MRCI)

在 U-Net 的第 $l$ 层，由于同步化设计，$Z_l$ 和 $F_l$ 具有相同的时间维度 $T_l$。我们应用标准的交叉注意力机制 [Vaswani et al., 2017]，其中 $Z_l$ 作为 Query，$F_l$ 作为 Key 和 Value。

我们显式地执行层级映射（MRCI）：

-   $F_{global} \rightarrow$ 瓶颈层 ($T_L$)：引导全局结构和长时程连贯性。
-   $F_{local} \rightarrow$ 中间层 ($T_M$)：约束局部和声进行和节奏模式。
-   $F_{event} \rightarrow$ 高分辨率层 ($T_H$)：确保精确的音符时序和力度还原。

##### 2.3.2 跳跃连接调制 (Skip Connection Modulation, SCM) (创新点)

U-Net 的跳跃连接负责将编码器路径的高频、高分辨率细节（包含丰富的瞬态信息）传递给解码器 [Ronneberger et al., 2015]。我们提出 SCM，直接使用符号信息调制跳跃连接本身，以强制高频细节与符号结构精确对齐。

令 $Z_{skip, l}$ 为从编码器第 $l$ 层传递的特征，$F_l$ 为对应的符号特征。我们计算调制后的跳跃连接 $Z'_{skip, l}$：

$$ Z'_{skip, l} = Z_{skip, l} + \text{GatedTCA}(Z_{skip, l}, F_l) $$

我们设计 GatedTCA 模块，它是一个标准的 TCA 模块，但其输出投影层（Output Projection Layer）被初始化为零。

$$ \text{GatedTCA}(Z, F) = W_{zero}(\text{TCA}(Z, F)) $$

其中 $W_{zero}$ 是一个初始化为零的线性层（或 1x1 卷积）。这种设计借鉴了 ControlNet [Zhang et al., 2023] 的思想。初始化为零确保了在训练初期 $\text{GatedTCA}(\cdot) \approx 0$，从而 $Z'_{skip, l} \approx Z_{skip, l}$。这保证了训练的稳定性，让模型逐渐学习如何利用这种强力的调制信号。在 SCM 中，TCA 使用 $Z_{skip, l}$ 作为 Query，以根据音频特征自身的内容自适应地从符号特征 $F_l$ 中提取所需的对齐信息。

#### 2.4 解耦的风格与结构控制

HS-MAD 架构天然支持结构（$C_H$）与风格/音色（$C_S$）的解耦。风格输入（文本或参考音频）通过预训练编码器（如 CLAP [Wu et al., 2023]）编码为全局风格向量 $F_{style}$。

我们采用自适应层归一化（AdaLN）[Dhariwal & Nichol, 2021; Peebles & Xie, 2023] 将 $F_{style}$ 和时间步嵌入 $t_{emb}$ 注入到 U-Net 的所有层级。一个调制网络（MLP）预测缩放参数 $\gamma_s$ 和偏移参数 $\beta_s$：

$$ \text{AdaLN}(Z, F_{style}, t_{emb}) = \gamma_s(F_{style}, t_{emb}) \cdot \text{LayerNorm}(Z) + \beta_s(F_{style}, t_{emb}) $$

这种全局注入方式与结构信息的注入（TCA 和 SCM）正交工作。

#### 2.5 训练目标与解耦 CFG

##### 2.5.1 训练目标

HS-MAD 的总训练目标结合了 LDM 损失 [Ho et al., 2020] 和 HSE 的辅助任务损失。

$$ L_{LDM} = \mathbb{E}_{z_0, t, \epsilon, C_H, C_S} \left[\|\epsilon - \epsilon_\theta(z_t, t, C_H, C_S)\|_2^2\right] $$

$$ L_{Total} = L_{LDM} + \sum_{Aux \in \{Harmony, Beat, Tempo, Key\}} \lambda_{Aux} \cdot L_{Aux} $$

##### 2.5.2 解耦无分类器指导 (Decoupled CFG) (创新点)

为了独立控制结构遵循度（Structural Adherence）和风格遵循度（Style Adherence），我们引入解耦 CFG 策略。

**训练阶段**：我们独立地随机丢弃这两种条件以支持 CFG [Ho & Salimans, 2022]。设置结构丢弃概率 $p_H$（例如 0.1）和风格丢弃概率 $p_S$（例如 0.1）。当条件被丢弃时，它们被替换为预学习的空嵌入（$\emptyset_H$ 或 $\emptyset_S$）。

**推理阶段**：我们使用加性 CFG（Additive CFG）公式 [Liu et al., 2022] 进行外插。我们分别控制结构引导强度 $w_H$ 和风格引导强度 $w_S$：

$$ \begin{aligned} \epsilon_{final} = \epsilon_{\theta}(z_t, \emptyset_H, \emptyset_S) &+ w_{H} \cdot (\epsilon_{\theta}(z_t, C_H, \emptyset_S) - \epsilon_{\theta}(z_t, \emptyset_H, \emptyset_S)) \\ &+ w_{S} \cdot (\epsilon_{\theta}(z_t, \emptyset_H, C_S) - \epsilon_{\theta}(z_t, \emptyset_H, \emptyset_S)) \end{aligned} $$

这种机制允许用户精确调整生成结果的侧重点（例如，高 $w_H$ 实现严格的乐谱遵循，调整 $w_S$ 探索不同音色）。该方法在推理时需要进行 3 次 U-Net 前向传播。

### 3. 算法细节与实现

本节提供核心模块的详细伪代码和具体的实现参数。

#### 3.1 算法伪代码

**算法 2：MRCI 1D U-Net 解码器块（带 SCM 和风格注入）**

```python
import torch
import torch.nn as nn

class GatedTemporalCrossAttention(nn.Module):
    # 实现带有零初始化输出层的 TCA (GatedTCA)，用于 SCM
    def __init__(self, Query_Dim, KeyValue_Dim):
        super().__init__()
        # 假设 StandardCrossAttention 已实现，处理 (B, T, D) 输入
        self.Attention = StandardCrossAttention(Query_Dim, KeyValue_Dim)
        # 输出投影层 (W_zero)，必须初始化为零，实现门控效果
        self.OutputProj_W_zero = nn.Linear(Query_Dim, Query_Dim)
        nn.init.zeros_(self.OutputProj_W_zero.weight)
        nn.init.zeros_(self.OutputProj_W_zero.bias)

    def forward(self, Query, KeyValue):
        # Query: (B, T, Query_Dim), KeyValue: (B, T, KeyValue_Dim)
        AttnOut = self.Attention(Query, KeyValue)
        # 训练初期，输出接近于零
        return self.OutputProj_W_zero(AttnOut)

class MRCI_DecoderBlock1D(nn.Module):
    # 假设 StandardUNetBlock1D, AdaLN_Modulator 已实现
    def __init__(self, C_in, C_out, C_skip, D_l, D_style):
        super().__init__()
        self.UNetBlock = StandardUNetBlock1D(C_in, C_out)
        self.StyleModulator = AdaLN_Modulator(D_style, C_out)
        
        # SCM 使用 GatedTCA。Query维度是 Z_skip 的通道数 C_skip。
        self.SCM_GatedTCA = GatedTemporalCrossAttention(Query_Dim=C_skip, KeyValue_Dim=D_l)
        
        # MRCI 使用标准 TCA。Query维度是 Z 的通道数 C_out。
        self.MRCI_TCA = StandardCrossAttention(Query_Dim=C_out, KeyValue_Dim=D_l)
        
        # 输入融合投影层
        self.InputProj = nn.Conv1d(C_in + C_skip, C_out, 1)

    def forward(self, Z_in, Z_skip, F_l, t_emb, F_style):
        """
        Args:
            Z_in: 上一层上采样/处理后的特征图 [B, C_in, T_l]。
            Z_skip: 来自编码器路径的跳跃连接特征图 [B, C_skip, T_l]。
            F_l: 当前层级的符号特征 [B, T_l, D_l]。
            t_emb, F_style: 时间步和风格嵌入。
        Returns:
            Z_out: 处理后的特征图 [B, C_out, T_l]。
        """
        # 维度断言：确保时间对齐
        T_l = Z_skip.shape[2]
        assert T_l == F_l.shape[1] == Z_in.shape[2]

        # 1. 跳跃连接调制 (SCM)
        # 将 Z_skip (B, C_skip, T_l) 调整为 (B, T_l, C_skip)
        Z_skip_reshaped = Z_skip.transpose(1, 2)

        # 使用 GatedTCA 进行调制。Query 来自 Z_skip, Key/Value 来自 F_l。
        # 由于 GatedTCA 的零初始化 (W_zero)，训练初期 Modulation 接近于零。
        Modulation = self.SCM_GatedTCA(Query=Z_skip_reshaped, KeyValue=F_l) # (B, T_l, C_skip)
        
        # 恢复维度 (B, C_skip, T_l) 并应用调制
        Z_skip_modulated = Z_skip + Modulation.transpose(1, 2)

        # 2. 特征融合与投影
        Z = torch.cat([Z_in, Z_skip_modulated], dim=1) # 沿特征维度 (C) 拼接
        Z = self.InputProj(Z)

        # 3. 基础 U-Net 计算与时间步注入
        Z = self.UNetBlock(Z, t_emb)

        # 4. 风格注入 (AdaLN)
        Z = self.StyleModulator(Z, F_style)

        # 5. 结构注入 (MRCI)
        # 使用标准 TCA 注入结构信息 F_l，强化结构约束。Query 来自 Z。
        Z_reshaped = Z.transpose(1, 2) # (B, T_l, C_out)
        Structural_Guidance = self.MRCI_TCA(Query=Z_reshaped, KeyValue=F_l)
        
        # 使用残差连接，并恢复维度 (B, C_out, T_l)
        Z_out = Z + Structural_Guidance.transpose(1, 2)

        return Z_out
```

#### 3.2 实现细节 (Implementation Details)

本节提供实现 HS-MAD 所需的关键参数和配置，确保可复现性。

##### 3.2.1 音频编解码器与预处理

-   **编解码器**: 我们采用 Descript Audio Codec (DAC) [Kumar et al., 2023] @ 44.1kHz。我们使用其无量化的连续潜在表示（Continuous Latents）作为 LDM 的训练目标，以获得最高保真度。
-   **配置**: DAC 配置为特征维度 $C=1024$。时间压缩因子 $R_{VAE}=320$。
-   **潜在空间分辨率**: $SR_{latent} = 44100 / 320 \approx 137.8$ Hz。
-   **数据处理**: 音频被切分为 30 秒的片段进行训练。$T_H = \lfloor 30 \times 137.8 \rfloor = 4134$。

##### 3.2.2 HSE 架构参数

-   **SRM**: 输入维度 $D_{in}=128 \times 3 = 384$（Onset, Sustain, Velocity）。渲染参数 $\sigma_{on} = \sigma_{off} = 1 / (2 \cdot SR_{latent}) \approx 3.6$ ms。
-   **HFE (Conformer)**:
    -   采用相对位置编码 [Huang et al., 2018]。
    -   Conformer 块配置：8 个注意力头，FFN 维度 2048，卷积核大小 31。
    -   维度：$D_{event}=512, D_{local}=1024, D_{global}=1536$。
    -   层数：每个阶段 6 个 Conformer 块。
-   **下采样因子**: 与 U-Net 匹配。设置 $R_1=4, R_2=4$。
    -   $T_H=4134$ (137.8 Hz)
    -   $T_M \approx 1033$ (34.4 Hz)
    -   $T_L \approx 258$ (8.6 Hz)
-   **辅助任务**: 标签数据使用 `music21` 和 `Madmom` [Böck et al., 2016] 库提取，并重采样到对应的分辨率 $T_M$ 和 $T_L$。

##### 3.2.3 MRCI 1D U-Net 架构参数

-   **架构**: 采用类似于 Stable Audio [Sors et al., 2024] 的 1D U-Net 架构，结合了 1D ResNet 块和 Transformer 块（包含自注意力和交叉注意力 TCA）。
-   **层级结构**: 采用对称结构，包含 3 个分辨率级别 $T_H, T_M, T_L$。下采样因子配置为 ($R_1=4, R_2=4$)。总下采样因子 16。
-   **维度**: U-Net 通道数配置为 {High-Res: 512, Mid-Res: 1024, Low-Res/Bottleneck: 1536}。
-   **SCM**: 在 High-Res 和 Mid-Res 级别应用 SCM。GatedTCA 模块的输出投影层 ($W_{zero}$) 初始化为零。

##### 3.2.4 训练超参数

-   **优化器**: AdamW [Loshchilov & Hutter, 2019]，$\beta_1=0.9, \beta_2=0.999$, weight decay 0.01。
-   **学习率**: 峰值学习率 $1e-4$，使用线性预热（Warmup）10000 步，随后使用余弦衰减。
-   **Batch Size**: 在 8 个 A100 GPU 上训练，全局 Batch Size 为 128（每个 GPU 16 个样本）。
-   **训练步数**: 1M 步。
-   **EMA**: 使用指数移动平均（EMA）[Polyak & Juditsky, 1992] 更新模型权重，衰减率 0.9999。
-   **CFG Dropout**: $p_H=0.1, p_S=0.1$。
-   **辅助损失权重**: $\lambda_{Aux} = 0.1$（对于所有辅助任务）。

### 4. 实验设计与验证

实验的核心目标是证明 HS-MAD 在音频质量和结构一致性上均优于现有方法，并且验证其核心创新模块的有效性。

#### 4.1 数据集与预处理

我们使用以下高质量、时间对齐的（音频，MIDI）数据集：

-   **MAESTRO v3.0.0** [Hawthorne et al., 2019]：约 200 小时古典钢琴演奏录音，具有毫秒级对齐的 MIDI 数据。用于评估精确时序和表现力。
-   **Slakh2100** [Manilow et al., 2019]：145 小时多乐器音乐的合成音频和对齐 MIDI。用于验证模型在复杂编曲和多音色场景下的泛化能力。

**预处理**：如 3.2.1 节所述。风格标签（$C_S$）对于 MAESTRO 统一为“Classical Piano Performance”，对于 Slakh 则使用乐器组合和流派作为文本描述。

#### 4.2 评估指标

我们将从三个维度全面评估模型性能：

1.  **音频保真度 (Fidelity)**：

    -   **Fréchet Audio Distance (FAD)** [Kilgour et al., 2019]：评估音色真实感和音频质量。
    -   **主观听觉测试 (MUSHRA)**：评估生成的音乐表现力、自然度和整体听感。

2.  **结构一致性 (Structural Adherence) [核心指标]**：

    我们使用最先进的 AMT 模型 MT3 [Gardner et al., 2022] 将生成音频转录回 MIDI，并与输入的 Ground Truth MIDI 进行比较。

    -   **音符 F1 分数 (Note F1 Score)**：衡量音符事件（Onset, Pitch）的精确度。采用严格的时序容差（50ms）进行评估 [Benetos et al., 2018]。
    -   **时序对齐误差 (Temporal Alignment Error, TAE)**：量化生成的音符与输入音符之间的时序漂移。定义为匹配的音符对的起始时间差的均方根（RMSE）：
        $$ TAE = \sqrt{\frac{1}{N_{match}} \sum_{i=1}^{N_{match}} (t_{gen, i} - t_{gt, i})^2} $$
    -   **力度相关性 (Velocity Correlation)**：评估模型还原输入动态的能力（皮尔逊相关系数）。

3.  **风格遵循度 (Style Adherence)**：

    -   **CLAP 分数** [Wu et al., 2023]：计算生成音频与文本风格提示之间的语义相似度。

#### 4.3 关键实验与假设

1.  **SOTA 性能对比**：
    -   **基线模型**：
        (a) **全局条件 LDM (Baseline-Global)**：仅将符号信息编码为全局向量，注入 U-Net 瓶颈层。
        (b) **扁平条件 LDM (Baseline-Flat)**：仅将高分辨率符号特征 $F_{event}$ 注入 U-Net 高分辨率层（无层级结构）。
        (c) **符号条件 MusicGen** [Copet et al., 2023]：最先进的自回归模型。
    -   **假设**：HS-MAD 在结构一致性指标（Note F1, TAE）上显著优于所有基线，证明层级对齐的必要性。同时，HS-MAD 的 FAD 分数将优于 MusicGen，并与 LDM 基线相当或更优。

2.  **消融实验 (Ablation Studies)**：
    -   **MRCI 的影响**：移除不同层级的注入。假设移除 $F_{event}$ 导致 TAE 显著增加（时序模糊）；移除 $F_{global}$ 导致长时程结构漂移（TAE 随时间累积增加）。
    -   **跳跃连接调制 (SCM)**：移除 SCM 机制（使用标准跳跃连接，移除 GatedTCA）。假设移除 SCM 将导致 Onset F1 分数下降，瞬态模糊。
    -   **HSE 辅助任务**：移除辅助损失 ($L_{Aux}=0$)。假设这将导致 HSE 特征层级解耦能力下降，并导致 MRCI 效果减弱。
    -   **SRM 渲染方式**：将 SRM 的软渲染替换为二值化钢琴卷帘。假设这将损失微观时序信息，导致 TAE 增加，听感机械化。

3.  **解耦控制与风格迁移**：
    -   在 Slakh 数据集上，固定 MIDI 输入，改变风格提示（例如，“Jazz Trio” vs “Symphonic Orchestra”）。
    -   **评估**：分析结构保持度（Note F1）和风格遵循度（CLAP Score）。测试 Decoupled CFG，分析不同 $w_H$ 和 $w_S$ 设置下生成结果在 F1-CLAP 空间中的权衡曲线（Pareto Front）。

### 5. 结论

本文提出了 HS-MAD，一个创新的层级符号对齐 1D 扩散模型，用于实现高保真且结构精确可控的音乐音频生成。HS-MAD 的核心理论基础是利用音乐层级结构与 U-Net 多分辨率特性之间的同构性，通过“分而治之”的策略解决了保真度与可控性之间的矛盾。我们详细设计了分层符号编码器（HSE）和同步渲染模块（SRM），确保了精确的时间对齐，并通过辅助任务实现了特征的层级解耦。多分辨率条件注入（MRCI）策略实现了有效的结构引导。特别地，创新的跳跃连接调制（SCM）机制，通过零初始化的 GatedTCA，显著增强了音频瞬态的时序精度。此外，解耦无分类器指导（Decoupled CFG）策略提供了灵活的结构与风格独立控制。通过详尽的算法设计和实现细节，HS-MAD 为 AI 音乐合成提供了一个强大、灵活且理论完备的新范式。

---

### 参考文献 (References)

[Benetos et al., 2018] Benetos, E., et al. (2018). Automatic music transcription: An overview. *IEEE Signal Processing Magazine*.

[Böck et al., 2016] Böck, S., et al. (2016). Madmom: A new Python audio and music signal processing toolbox. *Proceedings of the ACM International Conference on Multimedia*.

[Copet et al., 2023] Copet, J., et al. (2023). Simple and Controllable Music Generation. *NeurIPS*. (MusicGen)

[Défossez et al., 2022] Défossez, A., et al. (2022). High Fidelity Neural Audio Compression. *arXiv:2210.13438*. (EnCodec)

[Dhariwal & Nichol, 2021] Dhariwal, P., & Nichol, A. (2021). Diffusion Models Beat GANs on Image Synthesis. *NeurIPS*. (AdaLN context)

[Gardner et al., 2022] Gardner, J., et al. (2022). MT3: Multi-Task Multitrack Music Transcription. *ICLR*.

[Gulati et al., 2020] Gulati, A., et al. (2020). Conformer: Convolution-augmented Transformer for Speech Recognition. *Interspeech*.

[Hawthorne et al., 2018] Hawthorne, C., et al. (2018). Onsets and Frames: Dual-Objective Piano Transcription. *ISMIR*. (Soft Piano Roll concept).

[Hawthorne et al., 2019] Hawthorne, C., et al. (2019). Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset. *ICLR*. (MAESTRO Dataset)

[Ho et al., 2020] Ho, J., Jain, A., & Abbeel, P. (2020). Denoising Diffusion Probabilistic Models. *NeurIPS*. (DDPM)

[Ho & Salimans, 2022] Ho, J., & Salimans, T. (2022). Classifier-Free Diffusion Guidance. *arXiv:2207.12598*. (CFG)

[Huang et al., 2018] Huang, C. Z. A., et al. (2018). Music transformer: Generating music with long-term structure. *ICLR*. (Relative Attention)

[Kilgour et al., 2019] Kilgour, K., et al. (2019). Fréchet Audio Distance: A Metric for Evaluating Music Enhancement Algorithms. *Interspeech*. (FAD)

[Kumar et al., 2023] Kumar, R. T., et al. (2023). High-Fidelity Audio Compression with Improved RVQGAN. *NeurIPS*. (Descript Audio Codec - DAC)

[Lerdahl & Jackendoff, 1983] Lerdahl, F., & Jackendoff, R. (1983). *A generative theory of tonal music*. MIT press.

[Liu et al., 2022] Liu, N., et al. (2022). Compositional Visual Generation with Composable Diffusion Models. *ECCV*. (Additive/Composable CFG).

[Liu et al., 2023] Liu, H., et al. (2023). AudioLDM: Text-to-Audio Generation with Latent Diffusion Models. *ICML*.

[Loshchilov & Hutter, 2019] Loshchilov, I., & Hutter, F. (2019). Decoupled Weight Decay Regularization. *ICLR*. (AdamW)

[Manilow et al., 2019] Manilow, E., et al. (2019). Cutting the Cord: The Slakh Dataset for Instrument-Specific Source Separation and Synthesis. *ISMIR*. (Slakh2100)

[Paiement et al., 2009] Paiement, J. F., Bengio, Y., & Douglas, E. (2009). A Hierarchical Model of Music. *ISMIR*.

[Peebles & Xie, 2023] Peebles, W., & Xie, S. (2023). Scalable Diffusion Models with Transformers. *ICCV*. (AdaLN reference in DiT)

[Polyak & Juditsky, 1992] Polyak, B. T., & Juditsky, A. B. (1992). Acceleration of stochastic approximation by averaging. *SIAM Journal on Control and Optimization*. (EMA)

[Rombach et al., 2022] Rombach, R., et al. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. *CVPR*. (LDM/Stable Diffusion)

[Ronneberger et al., 2015] Ronneberger, O., Fischer, P., & Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. *MICCAI*.

[Sors et al., 2024] Sors, A., et al. (2024). Stable Audio 1.0. *Stability AI Technical Report*.

[Vaswani et al., 2017] Vaswani, A., et al. (2017). Attention Is All You Need. *NeurIPS*.

[Wu et al., 2023] Wu, X., et al. (2023). Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation. *ICASSP*. (CLAP).

[Zhang et al., 2023] Zhang, L., et al. (2023). Adding Conditional Control to Text-to-Image Diffusion Models. *ICCV*. (ControlNet)