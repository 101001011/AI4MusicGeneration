## HS-MAD：层级符号对齐的音乐音频扩散模型

**Hierarchical Symbolic-Conditioned Music Audio Diffusion (HS-MAD)**

### 摘要

在音乐音频生成领域，实现高保真度（Fidelity）与精确结构控制（Controllability）的平衡是长期存在的挑战。潜在扩散模型（LDM）虽然在音质上达到了最先进水平 [Rombach et al., 2022; Liu et al., 2023]，但由于缺乏显式的、时间对齐的结构化引导机制，它们在遵循复杂符号指令（如乐谱）时常出现时序漂移和结构失真。本文提出 HS-MAD（Hierarchical Symbolic-Conditioned Music Audio Diffusion），一种新颖的扩散架构，旨在解决这一核心矛盾。HS-MAD 的核心洞察在于利用音乐的层级本质（从宏观曲式到微观音符）[Paiement et al., 2009] 与 U-Net 架构的多分辨率特性 [Ronneberger et al., 2015] 之间的同构性（Isomorphism）。我们引入了一个基于 Conformer 的分层符号编码器（HSE），通过一个同步渲染模块（SRM）确保输入特征与 U-Net 潜在空间精确对齐。随后，我们设计了多分辨率条件注入（MRCI）机制，将高层结构信息引导 U-Net 的低分辨率层，将底层时序信息引导高分辨率层。我们进一步提出了“跳跃连接调制”（Skip Connection Modulation, SCM），通过直接调制 U-Net 的跳跃连接来显著增强音频瞬态的时序精度。此外，引入了“解耦无分类器指导”（Decoupled CFG）策略以实现结构与风格的独立控制，以及层级辅助任务来强化 HSE 的特征解耦。本文提供了详尽的架构设计和算法细节，论证了 HS-MAD 在实现高质量、结构连贯的音乐生成方面的潜力。

### 1. 引言：动机与贡献

音乐是一种高度结构化且具有复杂层级特性的时序信号 [Paiement et al., 2009; Lerdahl & Jackendoff, 1983]。生成模型在尝试复现这种复杂性时，必须同时满足声学保真度和音乐结构连贯性的要求。

近年来，潜在扩散模型（LDM）[Rombach et al., 2022] 在音频生成领域取得了突破性进展。AudioLDM [Liu et al., 2023] 和 Stable Audio [Sors et al., 2024] 等模型能够生成高度逼真的音色和织体。然而，它们的控制机制通常依赖于全局嵌入向量（例如来自文本提示的全局 CLAP 嵌入 [Wu et al., 2023]）。这种粗粒度的控制无法精确指定音乐的局部时间结构、和声进行或复调关系。在执行符号到音频（Symbol-to-Audio）合成任务时，现有的 LDM 尝试将符号信息（如 MIDI）编码后，常采用“单点注入”策略，通常注入到 U-Net 的瓶颈层或通过交叉注意力注入到单一层级。这造成了严重的信息瓶颈，尤其是在处理长时程结构时，导致时序漂移（Temporal Drift）、音符丢失和细节失真。

HS-MAD 旨在通过一个关键的理论洞察来解决这一矛盾：**音乐的层级结构与 U-Net 架构的多分辨率特性之间存在天然的同构性。** U-Net [Ronneberger et al., 2015] 的深层（低分辨率、大感受野）负责捕捉全局上下文和宏观结构，而浅层（高分辨率、小感受野）负责处理局部细节和纹理。这与音乐从曲式、和声进行到具体音符事件和瞬态的层级组织高度吻合。

HS-MAD 首次显式地将这两种层级结构进行对齐和映射。我们提出了一种“分而治之”的策略：利用高层音乐结构信息引导 U-Net 的深层计算，确保全局连贯性；利用底层音乐细节信息引导 U-Net 的浅层计算，确保微观时序精度。

本文的主要贡献与创新点如下：

1.  **[核心创新] HS-MAD 框架**：提出了通过显式对齐音乐层级结构与 U-Net 多分辨率特性来实现精确结构控制的音乐音频生成框架。
2.  **[创新点] 分层符号编码器 (HSE) 与同步机制**：设计了一个基于 Conformer [Gulati et al., 2020] 的层级化编码器，并结合了精确的同步渲染模块（SRM），以生成与 U-Net 特征图在时间维度上严格对齐的多尺度表示。我们还引入了层级辅助任务来强化特征解耦。
3.  **[创新点] 多分辨率条件注入 (MRCI) 与跳跃连接调制 (SCM)**：开发了层级注入策略，并创新性地引入了 SCM，直接使用符号信息调制 U-Net 的跳跃连接，以显著增强音频瞬态的清晰度和时序精度。
4.  **[创新点] 解耦无分类器指导 (Decoupled CFG)**：提出了一种针对结构和风格条件的多模态 CFG [Ho & Salimans, 2022] 策略，允许独立控制不同条件的引导强度。

### 2. HS-MAD 架构详解

HS-MAD 基于 LDM 框架 [Rombach et al., 2022]，包含一个预训练的音频自编码器（例如 VAE 或神经编解码器，如 EnCodec [Défossez et al., 2022] 或 DAC [Kumar et al., 2023]）（编码器 $\mathcal{E}$，解码器 $\mathcal{D}$），以及核心的去噪网络 $\epsilon_\theta$。去噪网络由分层符号编码器 (HSE) 和多分辨率条件注入 U-Net (MRCI U-Net) 组成。

HS-MAD 的目标是学习从噪声 $z_T \sim \mathcal{N}(0, I)$ 生成潜在表示 $z_0$ 的反向扩散过程，该过程受符号条件 $C_H$ 和风格条件 $C_S$ 的引导。最终音频 $X$ 通过 $X = \mathcal{D}(z_0)$ 获得。

```
graph TD
    subgraph "HS-MAD 详细架构"
        direction TB

        %% 1. 输入与编码
        Symbolic_Input("符号输入 S (MIDI)") --> HSE
        Style_Input("风格输入 T (文本/音频参考)") --> Style_Encoder("风格编码器 (CLAP/T5)") --> F_style("F_style (C_S)")

        subgraph HSE [分层符号编码器 (HSE)]
            direction TB
            S_in("S") --> SRM("同步渲染模块 (SRM)") --> S_sync("同步表示 S_sync (T_H)")
            S_sync --> Conformer_1("Stage 1: Event") --> F_event("F_event (T_H)")
            F_event -- "Strided Conv (R1)" --> Conformer_2("Stage 2: Local") --> F_local("F_local (T_M)")
            F_local -- "Strided Conv (R2)" --> Conformer_3("Stage 3: Global") --> F_global("F_global (T_L)")
            
            %% Auxiliary Tasks (创新点)
            F_local -- "Aux Head" --> L_aux_local("L_aux (和声)")
            F_global -- "Aux Head" --> L_aux_global("L_aux (速度)")
        end
        
        C_H("C_H = {F_event, F_local, F_global}")

        %% 2. MRCI U-Net (去噪网络 ε_θ)
        Noisy_Latent_zt("带噪潜在表示 z_t (T_H)") --> MRCI_U_Net

        subgraph "MRCI U-Net"
            direction TB
            Input_zt("z_t") --> U_Down_1("下采样块 1 (T_H)")
            U_Down_1 -- "Downsample (R1)" --> U_Down_2("下采样块 2 (T_M)")
            U_Down_2 -- "Downsample (R2)" --> U_Bottleneck("瓶颈层 (T_L)")

            U_Bottleneck --> U_Up_2("上采样块 2 (T_M)")
            U_Up_2 --> U_Up_1("上采样块 1 (T_H)")
            U_Up_1 --> Output_e("预测噪声 ε_θ")

            %% 跳跃连接调制 (SCM) (创新点)
            U_Down_1 -- "Z_skip_1" --> Skip_Mod_1("SCM (Gated TCA)")
            F_event -- "Condition" --> Skip_Mod_1
            Skip_Mod_1 -- "Z'_skip_1" --> U_Up_1

            U_Down_2 -- "Z_skip_2" --> Skip_Mod_2("SCM (Gated TCA)")
            F_local -- "Condition" --> Skip_Mod_2
            Skip_Mod_2 -- "Z'_skip_2" --> U_Up_2

            %% 多分辨率条件注入 (MRCI)
            F_event -- "TCA" --> U_Down_1 & U_Up_1
            F_local -- "TCA" --> U_Down_2 & U_Up_2
            F_global -- "TCA" --> U_Bottleneck

            %% 风格注入
            F_style -- "AdaLN/FiLM" --> U_Down_1 & U_Down_2 & U_Bottleneck & U_Up_2 & U_Up_1
        end
    end
```

*图 1：HS-MAD 架构概览。HSE 通过 SRM 实现与 U-Net 潜在空间的精确时间同步，并通过 Conformer 提取多尺度特征，其训练由辅助任务引导。MRCI U-Net 通过多分辨率条件注入 (MRCI) 和跳跃连接调制 (SCM) 机制，在对应层级利用这些特征进行引导。风格信息通过全局注入实现。*

#### 2.1 分层符号表示定义

我们定义三个层次的音乐特征，其时间分辨率与 U-Net 的层级精确对齐。令 U-Net 的输入（VAE 潜在空间）分辨率为 $T_H$。我们定义 U-Net 内部的下采样因子 $R_1$ 和 $R_2$，使得中分辨率 $T_M = T_H / R_1$，低分辨率（瓶颈层）$T_L = T_M / R_2$。

1.  **$F_{event}$ (事件层, $T_H$)**: 捕捉精确的音符属性，包括音高、力度、起始时间（Onset）、持续时间和微观时序（Micro-timing）。
2.  **$F_{local}$ (局部层, $T_M$)**: 捕捉局部和声（如和弦进行、调性中心）和节奏模式。
3.  **$F_{global}$ (全局层, $T_L$)**: 捕捉宏观曲式结构、全局调性和速度曲线。

#### 2.2 分层符号编码器 (HSE) (创新点)

HSE 的关键挑战在于将异步的符号事件 $S$（如 MIDI）转换为与 U-Net 特征图同步对齐、且语义解耦的多尺度表示 $C_H = \{F_{event}, F_{local}, F_{global}\}$。我们采用一个三阶段架构：同步渲染模块（SRM）、层级特征提取器（HFE）和辅助任务引导。

##### 2.2.1 同步渲染模块 (SRM)：精确时间对齐

SRM 负责将 MIDI 事件 $S$ 转换为一个同步的、高分辨率的输入表示 $S_{sync} \in \mathbb{R}^{T_H \times D_{in}}$。这里的关键是时间维度 $T_H$ 的精确计算和对齐。

令音频自编码器（VAE/Codec）的时间压缩因子为 $R_{VAE}$。对于时长为 $DUR$ 秒、采样率为 $SR$ 的音频，其潜在空间的时间步数为：

$$ T_H = \lfloor (DUR \times SR) / R_{VAE} \rfloor $$

SRM 必须生成恰好 $T_H$ 个时间帧。我们将 $S_{sync}$ 实现为一种“软钢琴卷帘”（Soft Piano Roll）。其特征维度 $D_{in}$ 包含音高激活（按音高索引，例如 128 维）、力度、起始信号和持续信号。

为了保留微观时序并确保可微性，SRM 使用平滑窗口函数（如高斯核）来渲染音符事件，而不是使用二值化的钢琴卷帘。例如，对于一个音符 $n$，其音高为 $p_n$，起始时间为 $t_{start, n}$（秒），力度为 $v_n$。它在 $S_{sync}$ 的起始信号通道上的表示可以建模为：

$$ S_{sync}[t_h, \text{onset}, p_n] = v_n \cdot \exp\left(-\frac{(t_h \cdot \frac{R_{VAE}}{SR} - t_{start, n})^2}{2\sigma^2}\right) $$

其中 $t_h \in [0, T_H-1]$ 是时间帧索引，$t_h \cdot \frac{R_{VAE}}{SR}$ 将帧索引转换为实际时间（秒），$\sigma$ 控制渲染的平滑程度（锐度）。这种表示方式使得模型能够精确捕捉时序细节。

##### 2.2.2 层级特征提取器 (HFE)：基于 Conformer

HFE 从 $S_{sync}$ 中提取层级特征。我们采用 Conformer 架构 [Gulati et al., 2020]，它结合了卷积的局部建模能力（捕捉精确时序和局部模式）和 Transformer 的全局上下文捕捉能力（理解长程依赖）[Vaswani et al., 2017]。

HFE 由三个阶段组成，阶段之间通过时间下采样连接。关键要求是 HFE 的下采样因子 $R_1, R_2$ 必须与 U-Net 的结构精确匹配。我们使用步长卷积（Strided Convolution）进行下采样，因为它比简单的池化操作能更好地学习如何聚合时序信息。

##### 2.2.3 层级辅助任务 (Hierarchical Auxiliary Tasks) (创新点)

为了确保 HSE 提取的特征确实捕捉到了不同层次的音乐信息，并促进特征空间的层级解耦，我们引入了辅助任务来引导 HSE 的训练。

1.  **局部和声预测 (Local Harmony Prediction)**：在 $F_{local}$ 上添加一个轻量级分类头 $H_{local}$，用于预测当前时间窗口的和弦标签（Chord Label）或色度图（Chromagram）。使用交叉熵损失 $L_{Aux\_Harmony}$。
2.  **全局速度预测 (Global Tempo Prediction)**：在 $F_{global}$ 上添加一个回归头 $H_{global}$，用于预测全局 BPM 或速度曲线。使用均方误差损失 $L_{Aux\_Tempo}$。

这些辅助损失为 HSE 提供了明确的层级监督信号，促进了具有音乐意义的层级表示的学习。

**算法 1：分层符号编码器 (HSE) 伪代码**

```python
import torch
import torch.nn as nn

class HierarchicalSymbolicEncoder(nn.Module):
    # 伪代码，展示关键结构和维度处理逻辑
    def __init__(self, R_VAE, R_Unet_factors, SR, Dims):
        super().__init__()
        # R_Unet_factors (R1, R2) 必须与 U-Net 匹配
        self.R1, self.R2 = R_Unet_factors
        self.SRM = SymbolicRenderingModule(R_VAE, SR, Dims['in'])
        D_event, D_local, D_global = Dims['event'], Dims['local'], Dims['global']

        # HFE Stages (Conformer Blocks)
        # 注意：Conformer 实现通常接受 (B, T, D) 或 (B, D, T) 输入，此处假设 (B, D, T)
        self.Stage1_Event = ConformerBlocks(Dims['in'], D_event, ...)
        # 使用 Strided Conv 进行下采样
        self.Downsample1 = nn.Conv1d(D_event, D_local, stride=self.R1, ...)
        self.Stage2_Local = ConformerBlocks(D_local, D_local, ...)
        self.Downsample2 = nn.Conv1d(D_local, D_global, stride=self.R2, ...)
        self.Stage3_Global = ConformerBlocks(D_global, D_global, ...)

        # 辅助任务头
        self.AuxHead_Local = PredictionHead(D_local, ...) # 和弦预测
        self.AuxHead_Global = PredictionHead(D_global, ...) # 速度预测

    def forward(self, S_midi, Audio_Duration, Targets=None):
        # 1. 同步渲染模块 (SRM)
        # 计算目标分辨率 T_H 并渲染 S_sync
        # (B, D_in, T_H)
        S_sync = self.SRM(S_midi, Audio_Duration) 

        # 2. 层级特征提取器 (HFE)
        # Stage 1: T_H
        F_event = self.Stage1_Event(S_sync)

        # Stage 2: T_M
        H_mid = self.Downsample1(F_event)
        F_local = self.Stage2_Local(H_mid)

        # Stage 3: T_L
        H_global = self.Downsample2(F_local)
        F_global = self.Stage3_Global(H_global)

        # 3. 辅助任务损失计算 (仅训练时)
        Aux_Losses = {}
        if self.training and Targets is not None:
            # 计算损失时可能需要处理维度 (B, D, T) -> (B, T, D)
            Aux_Losses['Harmony'] = self.calculate_loss(
                self.AuxHead_Local(F_local), Targets['Harmony'])
            Aux_Losses['Tempo'] = self.calculate_loss(
                self.AuxHead_Global(F_global), Targets['Tempo'])

        # 返回时序维度在后的格式 (B, T, D) 以适应 U-Net Cross-Attention
        Features = (F_event.transpose(1, 2), F_local.transpose(1, 2), F_global.transpose(1, 2))
        return Features, Aux_Losses
```

#### 2.3 多分辨率条件注入 U-Net (MRCI U-Net) (核心创新点)

MRCI U-Net 是去噪网络 $\epsilon_\theta$。我们通过时序交叉注意力（TCA）将 HSE 的输出注入到 U-Net 的对应层级。

##### 2.3.1 时序交叉注意力 (TCA)

在 U-Net 的第 $l$ 层，给定特征图 $Z_l$ 和对应的符号特征 $F_l$。由于 HSE 的同步化设计（SRM 和匹配的下采样因子），$Z_l$ 和 $F_l$ 具有相同的时间维度 $T_l$。这使得我们可以应用标准的交叉注意力机制 [Vaswani et al., 2017]。

$$ \text{TCA}(Z_l, F_l) = \text{Softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V $$
$$ Q=W_Q Z_l, \quad K=W_K F_l, \quad V=W_V F_l $$

##### 2.3.2 分层注入策略 (MRCI)

我们显式地进行层级映射，实现“分而治之”的控制策略：

-   $F_{global}$ 注入瓶颈层：引导全局结构和长时程连贯性。
-   $F_{local}$ 注入中间层（上下采样路径）：约束局部和声进行和节奏模式。
-   $F_{event}$ 注入高分辨率层（上下采样路径）：确保精确的音符时序和力度还原。

##### 2.3.3 跳跃连接调制 (Skip Connection Modulation, SCM) (创新点)

U-Net 的跳跃连接负责将编码器路径的高频、高分辨率细节传递给解码器 [Ronneberger et al., 2015]。这些细节对于音频瞬态（Transients，如音符起始 Onset）的清晰度和时序精度至关重要。我们提出 SCM，直接使用符号信息调制跳跃连接本身，以确保高频细节与符号结构精确对齐。

令 $Z_{skip, l}$ 为从编码器第 $l$ 层传递的特征，$F_l$ 为对应的符号特征。我们计算调制后的跳跃连接 $Z'_{skip, l}$：

$$ Z'_{skip, l} = Z_{skip, l} + \gamma_l \cdot \text{TCA}(Z_{skip, l}, F_l) $$

其中，TCA 使用 $Z_{skip, l}$ 作为 Query，以根据其自身内容自适应地从 $F_l$ 中提取所需的对齐信息。$\gamma_l$ 是一个可学习的门控标量，初始化为零。这种设计借鉴了 ControlNet [Zhang et al., 2023] 的思想，允许在训练初期平滑地引入控制，有助于训练的稳定性。特别地，使用 $F_{event}$ 调制最高分辨率的跳跃连接，可以极大地增强音符起始时间的精确性和清晰度。

#### 2.4 风格与音色控制

HS-MAD 架构天然支持结构（由 HSE 控制）与风格/音色（由文本 $T$ 或参考音频控制）的解耦。风格输入通过预训练编码器（如 CLAP [Wu et al., 2023] 或 T5 [Raffel et al., 2020]）编码为全局风格向量 $F_{style}$。我们采用自适应层归一化（AdaLN）[Dhariwal & Nichol, 2021; Peebles & Xie, 2023] 或特征级线性调制（FiLM）[Perez et al., 2018] 将 $F_{style}$ 注入到 U-Net 的所有层级。这种注入方式与结构信息的注入（TCA 和 SCM）正交工作。

#### 2.5 训练策略与解耦 CFG

##### 2.5.1 训练目标

HS-MAD 的总训练目标结合了标准的 LDM 损失和 HSE 的辅助任务损失。令层级化符号条件为 $C_H=\{F_{event}, F_{local}, F_{global}\}$，风格条件为 $C_S=F_{style}$。

LDM 损失函数为：

$$ L_{LDM} = \mathbb{E}_{z_0, t, \epsilon, C_H, C_S} \left[\|\epsilon - \epsilon_\theta(z_t, t, C_H, C_S)\|_2^2\right] $$

总损失函数为 LDM 损失与辅助损失的加权和：

$$ L_{Total} = L_{LDM} + \lambda_H \cdot L_{Aux\_Harmony} + \lambda_T \cdot L_{Aux\_Tempo} $$

其中 $\lambda_H$ 和 $\lambda_T$ 是平衡不同任务的超参数。

##### 2.5.2 解耦无分类器指导 (Decoupled CFG) (创新点)

为了独立控制结构遵循度（Structural Adherence）和风格遵循度（Style Adherence），我们引入解耦 CFG 策略。

**训练阶段**：我们必须使模型能够处理条件的缺失以支持 CFG [Ho & Salimans, 2022]。我们独立地随机丢弃这两种条件。以概率 $p_H$ 丢弃 $C_H$（替换为预学习的空嵌入 $\emptyset_H$），以概率 $p_S$ 丢弃 $C_S$（替换为 $\emptyset_S$）。这使得模型能够学习所有四种条件组合：$(\emptyset_H, \emptyset_S), (C_H, \emptyset_S), (\emptyset_H, C_S), (C_H, C_S)$。

**推理阶段**：我们使用加性 CFG（Additive CFG）公式 [Liu et al., 2022] 进行外插。该公式基于条件独立性假设，允许我们分别计算每种条件的引导方向，并进行线性组合。我们分别控制结构引导强度 $w_H$ 和风格引导强度 $w_S$：

$$ \begin{aligned} \epsilon_{final} = \epsilon_{\theta}(z_t, \emptyset_H, \emptyset_S) &+ w_{H} \cdot (\epsilon_{\theta}(z_t, C_H, \emptyset_S) - \epsilon_{\theta}(z_t, \emptyset_H, \emptyset_S)) \\ &+ w_{S} \cdot (\epsilon_{\theta}(z_t, \emptyset_H, C_S) - \epsilon_{\theta}(z_t, \emptyset_H, \emptyset_S)) \end{aligned} $$

这种机制允许用户精确调整生成结果的侧重点，例如，要求严格遵循乐谱（高 $w_H$），同时探索不同的音色（调整 $w_S$）。注意，该方法在推理时需要进行 3 次 U-Net 前向传播。

### 3. 算法细节与伪代码

本节提供 MRCI U-Net 核心解码器块的详细伪代码，展示如何集成 MRCI、跳跃连接调制（SCM）和风格注入。该伪代码遵循 PyTorch 惯例，特征图维度为 (Batch, Channel, Time) 或 (B, C, T)。

**算法 2：MRCI U-Net 解码器块（带 SCM 和风格注入）**

```python
import torch
import torch.nn.functional as F

def MRCI_DecoderBlock(Z_up, Z_skip, F_l, t_emb, F_style, gamma_l, StyleModulator):
    """
    MRCI U-Net 解码器块实现。
    Args:
        Z_up: 上一层上采样后的特征图 [B, C_up, T_l]。
        Z_skip: 来自编码器路径的跳跃连接特征图 [B, C_skip, T_l]。
        F_l: 当前层级的符号特征 (F_event 或 F_local) [B, T_l, D_l]。 (注意维度顺序)
        t_emb: 时间步嵌入。
        F_style: 全局风格嵌入 [B, D_style]。
        gamma_l: SCM 的可学习门控标量 (初始化为 0)。
        StyleModulator: 用于注入风格的模块 (例如 AdaLN 预测网络)。
    Returns:
        Z_out: 处理后的特征图 [B, C_out, T_l]。
    """
    # 维度断言：确保 HSE 输出与 U-Net 特征图时间对齐
    T_l = Z_skip.shape[2]
    assert T_l == F_l.shape[1] == Z_up.shape[2]

    # 1. 跳跃连接调制 (SCM) (创新点)
    # 将 Z_skip (B, C_skip, T_l) 调整为适合 Cross-Attention 的维度 (B, T_l, C_skip)
    Z_skip_reshaped = Z_skip.transpose(1, 2)

    # 使用 TCA 调制 Z_skip。Query 来自 Z_skip, Key/Value 来自 F_l。
    # TemporalCrossAttention 期望输入为 (B, T, D)
    Modulation = TemporalCrossAttention(Query=Z_skip_reshaped, KeyValue=F_l) # (B, T_l, C_skip)
    
    # 使用门控参数 gamma_l 控制强度，并恢复维度 (B, C_skip, T_l)
    Z_skip_modulated = Z_skip + gamma_l * Modulation.transpose(1, 2)

    # 2. 特征融合 (标准 U-Net 操作)
    Z = torch.cat([Z_up, Z_skip_modulated], dim=1) # 沿特征维度 (C) 拼接

    # 3. 基础 U-Net 计算 (例如，ResNet/Transformer 块)
    # 注入时间步信息 t_emb
    Z = StandardUNetBlock(Z, t_emb)

    # 4. 风格注入 (Style Injection)
    # 例如使用 AdaLN。StyleModulator 预测缩放和偏移参数。
    # gamma_s, beta_s = StyleModulator(F_style) # (B, C_out)
    # Z = F.layer_norm(Z, Z.shape[1:]) * (1 + gamma_s.unsqueeze(-1)) + beta_s.unsqueeze(-1)
    Z = StyleModulator(Z, F_style)

    # 5. 结构注入 (Structural Injection via MRCI)
    # 使用 TCA 再次注入结构信息 F_l，强化结构约束。Query 来自 Z。
    Z_reshaped = Z.transpose(1, 2) # (B, T_l, C_out)
    Structural_Guidance = TemporalCrossAttention(Query=Z_reshaped, KeyValue=F_l)
    
    # 使用残差连接，并恢复维度 (B, C_out, T_l)
    Z = Z + Structural_Guidance.transpose(1, 2)

    return Z
```

### 4. 实现可行性与关键技术点

#### 4.1 关键技术实现要点

1.  **精确时间同步的实现**：这是 HS-MAD 成功的基石。实现时必须精确计算 VAE/Codec 的总时间压缩因子 $R_{VAE}$，以及 U-Net 内部各层级的下采样因子 $R_1, R_2$。HSE 的 SRM 必须根据 $R_{VAE}$ 精确渲染 $S_{sync}$，且 HSE 内部的步长卷积步长必须严格等于对应的 U-Net 下采样因子。任何微小的实现偏差都可能导致性能严重下降。在代码中应加入大量的维度断言（如算法 1 和 2 所示）来确保对齐。

2.  **高质量音频编解码器选择**：LDM 的性能上限受限于自编码器的重建质量。为了实现 SOTA 级的保真度，建议采用最先进的神经音频编解码器，如 EnCodec [Défossez et al., 2022] 或 Descript Audio Codec (DAC) [Kumar et al., 2023]。它们提供了高保真度、良好的潜在空间特性和相对较低的下采样率。

3.  **SCM 的训练稳定性**：跳跃连接调制（SCM）是一个强力的控制机制。必须确保 $\gamma_l$ 初始化为零（或使用零卷积实现），以保证训练初期的稳定性，让模型逐渐学习如何利用这种调制，防止训练崩溃。

4.  **辅助任务的数据准备**：HSE 的辅助任务需要相应的标签数据（和弦、速度）。这些标签可以利用现有的音乐信息检索（MIR）工具（如 Librosa 或 music21）从 MIDI 数据中自动提取。这些标签需要与 HSE 对应层级的时间分辨率对齐，作为训练目标 $Targets$（见算法 1）。

#### 4.2 可行性论证

HS-MAD 建立在成熟的 LDM [Rombach et al., 2022] 和 Conformer [Gulati et al., 2020] 框架之上，具有高度的可行性。其核心思想（层级对齐）具有强大的理论支撑，将复杂的生成任务分解为在不同尺度上更容易管理的子任务。MAESTRO [Hawthorne et al., 2019] 和 Slakh2100 [Manilow et al., 2019] 等高质量、时间对齐的数据集提供了必要的数据支持。与需要从零开始训练复杂系统的方案相比，HS-MAD 可以基于现有的开源 LDM 实现进行扩展，大大降低了实现难度。

### 5. 实验设计与验证

实验的核心目标是证明 HS-MAD 在音频质量和结构一致性上均优于现有方法，并且具有优越的控制灵活性。

#### 5.1 数据集

我们将在以下两个具有高质量时间对齐的（音频，MIDI）数据集上进行训练和评估：

-   **MAESTRO** [Hawthorne et al., 2019]：包含大量高精度对齐的钢琴演奏录音和 MIDI。用于验证模型在精确时序控制和表现力（Expressiveness）方面的能力。
-   **Slakh2100** [Manilow et al., 2019]：包含大量多乐器音乐的合成音频和对齐 MIDI。用于验证模型在复杂编曲和多音色场景下的泛化能力。

#### 5.2 评估指标

我们将从三个维度评估模型性能：

1.  **音频保真度 (Fidelity)**：

    -   **Fréchet Audio Distance (FAD)** [Kilgour et al., 2019]：评估音色真实感和音频质量的客观指标。
    -   **主观听觉测试 (MOS/MUSHRA)**：评估生成的音乐表现力、自然度和整体听感。

2.  **结构一致性 (Structural Adherence) [核心指标]**：

    为了客观量化生成的音频与输入符号指令的对齐程度，我们使用先进的自动音乐转录（AMT）模型（例如 MT3 [Gardner et al., 2022]）将生成音频转录回 MIDI，并与输入的 Ground Truth MIDI 进行比较。

    -   **音符 F1 分数 (Note F1 Score)**：衡量音符事件（Onset, Offset, Pitch）的精确度。我们将采用严格的时序容差（例如 50ms）进行评估 [Benetos et al., 2018]。
    -   **时序对齐误差 (Temporal Alignment Error, TAE)**：衡量生成的音符起始时间与输入音符起始时间之间的平均时序漂移（Mean Absolute Error）。
    -   **力度相关性 (Velocity Correlation)**：评估模型还原输入动态（Dynamics）的能力（皮尔逊相关系数）。

3.  **风格遵循度 (Style Adherence)**：

    -   **CLAP 分数** [Wu et al., 2023]：计算生成音频与文本风格提示之间的语义相似度。

#### 5.3 关键实验

1.  **SOTA 性能对比**：
    -   **基线模型**：
        (a) **全局条件 LDM**：仅将符号信息编码后注入 U-Net 瓶颈层（模拟传统方法）。
        (b) **扁平条件 LDM**：仅将高分辨率符号特征 $F_{event}$ 注入 U-Net 高分辨率层。
        (c) **自回归模型**：如 MusicGen [Copet et al., 2023] 的符号条件版本。
    -   **预期结果**：HS-MAD 在结构一致性指标（Note F1, TAE）上显著优于所有基线，同时保持 SOTA 级的 FAD 分数。

2.  **消融实验 (Ablation Studies)**：
    -   **MRCI 的影响**：移除不同层级的注入（例如，移除 $F_{event}$ 注入，预期时序模糊；移除 $F_{global}$，预期长时程结构漂移），验证分层注入策略的有效性。
    -   **跳跃连接调制 (SCM)**：移除 SCM 机制（设置 $\gamma_l=0$），验证其对瞬态渲染质量和 Onset F1 分数的影响。预期移除后 Onset 精度下降，瞬态模糊。
    -   **HSE 辅助任务**：移除辅助损失 ($L_{Aux}$)，分析 HSE 特征的可视化和模型性能，验证其对层级解耦的作用。
    -   **解耦 CFG**：对比 Decoupled CFG 与标准联合 CFG（Joint CFG, $w_H=w_S$）在控制灵活性和音频质量上的差异。

3.  **解耦控制与风格迁移**：
    -   固定 MIDI 输入，改变风格提示（例如，将同一段旋律渲染为“古典钢琴”和“爵士风琴”）。评估结构保持度（Note F1）和风格遵循度（CLAP Score）。分析不同 $w_H$ 和 $w_S$ 设置下生成结果的权衡曲线。

### 6. 结论

本文提出了 HS-MAD，一个创新的层级符号对齐扩散模型。通过显式地利用音乐层级结构与 U-Net 多分辨率特性之间的同构性，HS-MAD 解决了高保真音频合成与精确结构控制之间的核心矛盾。我们引入了基于 Conformer 的分层符号编码器（HSE）和同步渲染模块（SRM），并通过辅助任务实现了特征的层级解耦。多分辨率条件注入（MRCI）策略实现了“分而治之”的结构引导。创新的跳跃连接调制（SCM）机制显著增强了音频瞬态的时序精度。此外，解耦无分类器指导（Decoupled CFG）策略提供了灵活的结构与风格控制。HS-MAD 能够在生成的每一个抽象层次都受到精确的结构约束，为 AI 音乐合成和创作提供了一个强大而灵活的新范式。

---

### 参考文献 (References)

[Benetos et al., 2018] Benetos, E., et al. (2018). Automatic music transcription: An overview. *IEEE Signal Processing Magazine*.

[Copet et al., 2023] Copet, J., et al. (2023). Simple and Controllable Music Generation. *NeurIPS*. (MusicGen)

[Défossez et al., 2022] Défossez, A., et al. (2022). High Fidelity Neural Audio Compression. *arXiv:2210.13438*. (EnCodec)

[Dhariwal & Nichol, 2021] Dhariwal, P., & Nichol, A. (2021). Diffusion Models Beat GANs on Image Synthesis. *NeurIPS*. (AdaLN context)

[Gardner et al., 2022] Gardner, J., et al. (2022). MT3: Multi-Task Multitrack Music Transcription. *ICLR*.

[Gulati et al., 2020] Gulati, A., et al. (2020). Conformer: Convolution-augmented Transformer for Speech Recognition. *Interspeech*.

[Hawthorne et al., 2019] Hawthorne, C., et al. (2019). Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset. *ICLR*.

[Ho & Salimans, 2022] Ho, J., & Salimans, T. (2022). Classifier-Free Diffusion Guidance. *arXiv:2207.12598*. (CFG)

[Kilgour et al., 2019] Kilgour, K., et al. (2019). Fréchet Audio Distance: A Metric for Evaluating Music Enhancement Algorithms. *Interspeech*. (FAD)

[Kumar et al., 2023] Kumar, R. T., et al. (2023). High-Fidelity Audio Compression with Improved RVQGAN. *NeurIPS*. (Descript Audio Codec - DAC)

[Lerdahl & Jackendoff, 1983] Lerdahl, F., & Jackendoff, R. (1983). *A generative theory of tonal music*. MIT press.

[Liu et al., 2022] Liu, N., et al. (2022). Compositional Visual Generation with Composable Diffusion Models. *ECCV*. (Additive/Composable CFG).

[Liu et al., 2023] Liu, H., et al. (2023). AudioLDM: Text-to-Audio Generation with Latent Diffusion Models. *ICML*.

[Manilow et al., 2019] Manilow, E., et al. (2019). Cutting the Cord: The Slakh Dataset for Instrument-Specific Source Separation and Synthesis. *ISMIR*. (Slakh2100)

[Paiement et al., 2009] Paiement, J. F., Bengio, Y., & Douglas, E. (2009). A Hierarchical Model of Music. *ISMIR*.

[Peebles & Xie, 2023] Peebles, W., & Xie, S. (2023). Scalable Diffusion Models with Transformers. *ICCV*. (AdaLN reference in DiT)

[Perez et al., 2018] Perez, E., et al. (2018). FiLM: Visual Reasoning with a General Conditioning Layer. *AAAI*.

[Raffel et al., 2020] Raffel, C., et al. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. *JMLR*. (T5).

[Rombach et al., 2022] Rombach, R., et al. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. *CVPR*. (LDM/Stable Diffusion)

[Ronneberger et al., 2015] Ronneberger, O., Fischer, P., & Brox, T. (2015). U-Net: Convolutional Networks for Biomedical Image Segmentation. *MICCAI*.

[Sors et al., 2024] Sors, A., et al. (2024). Stable Audio 1.0. *Stability AI Technical Report*.

[Vaswani et al., 2017] Vaswani, A., et al. (2017). Attention Is All You Need. *NeurIPS*.

[Wu et al., 2023] Wu, X., et al. (2023). Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation. *ICASSP*. (CLAP).

[Zhang et al., 2023] Zhang, L., et al. (2023). Adding Conditional Control to Text-to-Image Diffusion Models. *ICCV*. (ControlNet)